{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2281d062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to PYTHONPATH automatically\n",
    "PROJECT_ROOT = r\"C:\\Users\\adib4\\OneDrive\\Documents\\Projets perso\\CongestionAI\\find_issues.ipynb\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.model_pipelines.dl_pipeline import train_model, evaluate, predict\n",
    "from src.utils.model_evaluation import evaluate_and_plot_block\n",
    "from src.utils.hist_baseline import historical_baseline_multi\n",
    "from src.utils.preprocessing import cyclical_encode, scale_features, encode_detectors\n",
    "from src.utils.sequences import create_nhits_sequences, NHitsDataset\n",
    "from src.utils.plots import plot_training_curves\n",
    "from src.model_pipelines.losses import (\n",
    "    SpikeWeightedMSELoss,\n",
    "    TwoTermSpikeLoss,\n",
    "    DeltaLoss,\n",
    "    LossConfig,\n",
    "    create_loss\n",
    ")\n",
    "\n",
    "from src.utils.crafted_features import (\n",
    "    SpikeFeatureConfig,\n",
    "    add_spike_features,\n",
    "    add_delta_features,\n",
    "    add_rolling_stats,\n",
    "    add_spike_labels,\n",
    "    add_lags_and_drop\n",
    ")\n",
    "\n",
    "\n",
    "from src.models.mlp_forecaster import MLPForecaster\n",
    "from src.models.n_hits import NHitsForecaster\n",
    "from src.models.tcn_forecaster import TCNForecaster\n",
    "from src.models.transformer_forecaster import TransformerForecaster\n",
    "\n",
    "FILE_PATH = \"prepared_data/preprocessed_full_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7870e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_csv(FILE_PATH)\n",
    "df_base[\"timestamp\"] = pd.to_datetime(df_base[\"timestamp\"])\n",
    "df_base[\"orig_idx\"] = df_base.index\n",
    "df_base = cyclical_encode(df_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521caec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"n_blocks\": 6, \"hidden_dim\": 512, \"history\": \"24h_hourly\", \"weather_lags\": \"sparse_24h\"}\n",
    "\n",
    "# Fixed params\n",
    "forecast_horizon = 8\n",
    "nb_detectors = 20\n",
    "years_split = [[2016, 2017, 2018, 2020, 2021, 2022, 2023, 2024], [2019], []]\n",
    "\n",
    "# Feature configs\n",
    "feature_cols_norm_base = [\n",
    "    \"temperature\", \"precipitation\", \"visibility\", \"congestion_index\", \"free_flow_speed\"\n",
    "]\n",
    "feature_cols_base = [\n",
    "    \"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\", \"month_sin\", \"month_cos\",\n",
    "    \"lon\", \"lat\", \"year\", \"season\",\n",
    "    \"temperature\", \"precipitation\", \"visibility\",\n",
    "    \"congestion_index\", \"free_flow_speed\"\n",
    "]\n",
    "\n",
    "h_offsets = list(range(24))\n",
    "weather_lags = list(range(0,-8,-1))\n",
    "spike_config = None\n",
    "w_lags = [-1, -2, -6, -8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c503858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dl_data_with_spikes(history_offsets, forecast_horizon, nb_detectors, df_base,\n",
    "                                years_split, feature_cols_norm, feature_cols_base,\n",
    "                                weather_lags, spike_config=None):\n",
    "    \"\"\"Extended data prep with optional spike features.\"\"\"\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    df_small = df_base[df_base[\"detector_id\"].isin(df_base[\"detector_id\"].unique()[:nb_detectors])].copy()\n",
    "    df_small = df_small.sort_values([\"detector_id\", \"timestamp\"])\n",
    "    \n",
    "    # Season encoding\n",
    "    df_small.loc[(df_small[\"month\"] <= 2) | (df_small[\"month\"] == 12), \"season\"] = 0\n",
    "    df_small.loc[(df_small[\"month\"] > 2) & (df_small[\"month\"] <= 5), \"season\"] = 1\n",
    "    df_small.loc[(df_small[\"month\"] > 5) & (df_small[\"month\"] <= 8), \"season\"] = 2\n",
    "    df_small.loc[(df_small[\"month\"] > 8) & (df_small[\"month\"] <= 11), \"season\"] = 3\n",
    "    \n",
    "    # Add spike features if configured\n",
    "    feature_cols = feature_cols_base.copy()\n",
    "    feature_cols_norm_full = feature_cols_norm.copy()\n",
    "    \n",
    "    if spike_config is not None:\n",
    "        print(f\"Adding spike features: deltas={spike_config.enable_deltas}, rolling={spike_config.enable_rolling_stats}\")\n",
    "        df_small = add_spike_features(df_small, spike_config)\n",
    "        spike_feature_cols = spike_config.get_feature_columns()\n",
    "        spike_norm_cols = spike_config.get_normalization_columns()\n",
    "        feature_cols = feature_cols + spike_feature_cols\n",
    "        feature_cols_norm_full = feature_cols_norm_full + spike_norm_cols\n",
    "        print(f\"  Added columns: {spike_feature_cols}\")\n",
    "    \n",
    "    # Detector encoding\n",
    "    df_small, det2idx = encode_detectors(df_small)\n",
    "    \n",
    "    # Add weather lag column names\n",
    "    if \"temperature\" in feature_cols:\n",
    "        feature_cols = feature_cols + [f\"temperature_lag_{lag}h\" for lag in weather_lags] \\\n",
    "            + [f\"precipitation_lag_{lag}h\" for lag in weather_lags] \\\n",
    "            + [f\"visibility_lag_{lag}h\" for lag in weather_lags]\n",
    "    \n",
    "    # Split\n",
    "    train = df_small[df_small[\"timestamp\"].dt.year.isin(years_split[0])].copy()\n",
    "    val = df_small[df_small[\"timestamp\"].dt.year.isin(years_split[1])].copy()\n",
    "    test = df_small[df_small[\"timestamp\"].dt.year.isin(years_split[2])].copy() if years_split[2] else None\n",
    "    \n",
    "    train = train.set_index(\"orig_idx\")\n",
    "    val = val.set_index(\"orig_idx\")\n",
    "    if test is not None:\n",
    "        test = test.set_index(\"orig_idx\")\n",
    "    \n",
    "    # Normalization\n",
    "    minmax_cols = [\"lon\", \"lat\", \"year\", \"season\"]\n",
    "    train, val, test, std_scaler, mm_scaler = scale_features(\n",
    "        train, val, test, feature_cols_norm_full, latlon_cols=minmax_cols\n",
    "    )\n",
    "    \n",
    "    # Weather lags\n",
    "    if \"temperature\" in feature_cols_base:\n",
    "        train = add_lags_and_drop(train, weather_lags)\n",
    "        val = add_lags_and_drop(val, weather_lags)\n",
    "        if test is not None:\n",
    "            test = add_lags_and_drop(test, weather_lags)\n",
    "    \n",
    "    # Drop NaNs from spike features\n",
    "    if spike_config is not None:\n",
    "        spike_cols_in_df = [c for c in spike_feature_cols if c in train.columns]\n",
    "        train = train.dropna(subset=spike_cols_in_df)\n",
    "        val = val.dropna(subset=spike_cols_in_df)\n",
    "        if test is not None:\n",
    "            test = test.dropna(subset=spike_cols_in_df)\n",
    "    \n",
    "    # Keep only needed columns (congestion_index is already in feature_cols)\n",
    "    keep_cols = feature_cols + [\"timestamp\", \"detector_id\", \"det_index\"]\n",
    "    keep_cols = [c for c in keep_cols if c in train.columns]\n",
    "    \n",
    "    train = train[keep_cols]\n",
    "    val = val[keep_cols]\n",
    "    if test is not None:\n",
    "        test = test[keep_cols]\n",
    "    \n",
    "    # Build sequences\n",
    "    X_train_hist, Y_train, idx_train, det_train = create_nhits_sequences(\n",
    "        train, feature_cols, history_offsets, forecast_horizon)\n",
    "    X_val_hist, Y_val, idx_val, det_val = create_nhits_sequences(\n",
    "        val, feature_cols, history_offsets, forecast_horizon)\n",
    "    \n",
    "    if test is not None:\n",
    "        X_test_hist, Y_test, idx_test, det_test = create_nhits_sequences(\n",
    "            test, feature_cols, history_offsets, forecast_horizon)\n",
    "    else:\n",
    "        X_test_hist, Y_test, idx_test, det_test = None, None, None, None\n",
    "    \n",
    "    print(f\"Sequences created. Features: {len(feature_cols)}, Train samples: {len(Y_train)}\")\n",
    "    \n",
    "    return (X_train_hist, Y_train, idx_train, det_train,\n",
    "            X_val_hist, Y_val, idx_val, det_val,\n",
    "            X_test_hist, Y_test, idx_test, det_test,\n",
    "            train, val, test, std_scaler, mm_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de09554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Sequences created. Features: 27, Train samples: 1068551\n"
     ]
    }
   ],
   "source": [
    "X_train_hist, Y_train, idx_train, det_train, \\\n",
    "                X_val_hist, Y_val, idx_val, det_val, \\\n",
    "                X_test_hist, Y_test, idx_test, det_test, \\\n",
    "                train, val, test, std_scaler, mm_scaler = prepare_dl_data_with_spikes(\n",
    "                    h_offsets, forecast_horizon, nb_detectors, df_base,\n",
    "                    years_split=years_split,\n",
    "                    feature_cols_norm=feature_cols_norm_base,\n",
    "                    feature_cols_base=feature_cols_base,\n",
    "                    weather_lags=w_lags,\n",
    "                    spike_config=None\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9521bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta stats (z-normalized):\n",
      "  Mean: 0.1639\n",
      "  Std:  0.1985\n",
      "  Median: 0.1011\n",
      "  90th percentile: 0.3797\n",
      "  95th percentile: 0.5263\n",
      "  99th percentile: 0.9592\n",
      "\n",
      "% flagged as spike with threshold=0.15: 36.4%\n",
      "\n",
      "Recommended threshold (90th pct): 0.380\n"
     ]
    }
   ],
   "source": [
    "# Check what 0.15 threshold means in your data\n",
    "train_ci = train[\"congestion_index\"]  # Already z-normalized\n",
    "\n",
    "# Compute deltas\n",
    "deltas = train_ci.diff().abs().dropna()\n",
    "\n",
    "print(f\"Delta stats (z-normalized):\")\n",
    "print(f\"  Mean: {deltas.mean():.4f}\")\n",
    "print(f\"  Std:  {deltas.std():.4f}\")\n",
    "print(f\"  Median: {deltas.median():.4f}\")\n",
    "print(f\"  90th percentile: {deltas.quantile(0.90):.4f}\")\n",
    "print(f\"  95th percentile: {deltas.quantile(0.95):.4f}\")\n",
    "print(f\"  99th percentile: {deltas.quantile(0.99):.4f}\")\n",
    "\n",
    "# What % is flagged as spike with current threshold?\n",
    "pct_flagged = (deltas > 0.15).mean() * 100\n",
    "print(f\"\\n% flagged as spike with threshold=0.15: {pct_flagged:.1f}%\")\n",
    "\n",
    "# Recommended: use 90th or 95th percentile\n",
    "recommended_threshold = deltas.quantile(0.90)\n",
    "print(f\"\\nRecommended threshold (90th pct): {recommended_threshold:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
