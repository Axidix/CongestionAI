{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df143668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to PYTHONPATH automatically\n",
    "PROJECT_ROOT = r\"C:\\Users\\adib4\\OneDrive\\Documents\\Projets perso\\CongestionAI\\find_issues.ipynb\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.model_pipelines.dl_pipeline import train_model, evaluate, predict\n",
    "from src.utils.model_evaluation import evaluate_and_plot_block\n",
    "from src.utils.hist_baseline import historical_baseline_multi\n",
    "from src.utils.preprocessing import cyclical_encode, scale_features, encode_detectors\n",
    "from src.utils.sequences import create_nhits_sequences, NHitsDataset\n",
    "from src.utils.plots import plot_training_curves\n",
    "from src.model_pipelines.losses import (\n",
    "    SpikeWeightedMSELoss,\n",
    "    TwoTermSpikeLoss,\n",
    "    DeltaLoss,\n",
    "    LossConfig,\n",
    "    create_loss\n",
    ")\n",
    "\n",
    "from src.utils.crafted_features import (\n",
    "    SpikeFeatureConfig,\n",
    "    add_spike_features,\n",
    "    add_lags_and_drop\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from src.models.n_hits import NHitsForecaster\n",
    "from src.models.tcn_forecaster import MultiHeadTCNForecaster\n",
    "\n",
    "FILE_PATH = \"prepared_data/preprocessed_full_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a26ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dl_experiment(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    X_train_hist,\n",
    "    Y_train,\n",
    "    train_det_idx,\n",
    "    X_val_hist,\n",
    "    Y_val,\n",
    "    val_det_idx,\n",
    "    X_test_hist,\n",
    "    Y_test,\n",
    "    test_det_idx,\n",
    "    device=\"cuda\",\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    grad_clip=1.0,\n",
    "    scheduler=None,\n",
    "    scaler=None,\n",
    "    exp_name=\"\",\n",
    "    patience=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the full deep-learning training pipeline.\n",
    "\n",
    "    - Builds model + dataloaders\n",
    "    - Trains the model\n",
    "    - Returns model, predictions, losses\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # DATALOADERS\n",
    "    # -------------------------\n",
    "    train_loader = DataLoader(\n",
    "        NHitsDataset(X_train_hist, Y_train, train_det_idx),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        NHitsDataset(X_val_hist, Y_val, val_det_idx),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    if X_test_hist is None or Y_test is None or test_det_idx is None:\n",
    "        test_loader = None\n",
    "    else:\n",
    "        test_loader = DataLoader(\n",
    "            NHitsDataset(X_test_hist, Y_test, test_det_idx),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # -------------------------\n",
    "    # TRAINING\n",
    "    # -------------------------\n",
    "    train_losses, val_losses, best_state = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler,\n",
    "        device=device,\n",
    "        num_epochs=epochs,\n",
    "        grad_clip=grad_clip,\n",
    "        patience=patience,\n",
    "    )\n",
    "\n",
    "    # Save losses to file\n",
    "    os.makedirs(f\"plots_training_dl/{broad_exp_name}/\", exist_ok=True)\n",
    "    with open(f\"plots_training_dl/{broad_exp_name}/losses_{exp_name}.txt\", \"w\") as f:\n",
    "        f.write(\"epoch,train_loss,val_loss\\n\")\n",
    "        for i, (t_loss, v_loss) in enumerate(zip(train_losses, val_losses)):\n",
    "            f.write(f\"{i+1},{t_loss:.6f},{v_loss:.6f}\\n\")\n",
    "\n",
    "    plot_training_curves(train_losses, val_losses, filename=f\"training_curve{exp_name}.png\", dir = f\"plots_training_dl/{broad_exp_name}/\")\n",
    "    #model.load_state_dict(best_state)\n",
    "    if test_loader is not None:\n",
    "        _, test_loss = evaluate(model, test_loader, criterion, device)\n",
    "        print(f\"Test Loss ({exp_name}): {test_loss:.4f}\")\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d415c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_eval_df(df, idx_seq, preds, horizon):\n",
    "    \"\"\"\n",
    "    df: the dataset (train, val or test)\n",
    "    idx_seq: array of starting indices returned by create_nhits_sequences\n",
    "    preds: model predictions (N, horizon)\n",
    "    \"\"\"\n",
    "\n",
    "    df_subset = df.loc[idx_seq].copy()\n",
    "    print(df.info())\n",
    "    print(df_subset.info())\n",
    "    print(len(df_subset), len(idx_seq), preds.shape)\n",
    "\n",
    "    eval_df = pd.DataFrame({\n",
    "        \"row_idx\": idx_seq,\n",
    "        \"timestamp\": df_subset[\"timestamp\"].values,\n",
    "        \"detector_id\": df_subset[\"detector_id\"].values,\n",
    "    })\n",
    "    \n",
    "\n",
    "    # Add predictions\n",
    "    for h in range(1, horizon + 1):\n",
    "        eval_df[f\"pred_{h}h\"] = preds[:, h-1].numpy()\n",
    "\n",
    "    # Add ground truth targets\n",
    "    for h in range(1, horizon + 1):\n",
    "        eval_df[f\"future_{h}h\"] = (\n",
    "            df.groupby(\"detector_id\")[\"congestion_index\"]\n",
    "              .shift(-h)\n",
    "              .loc[idx_seq]\n",
    "              .values\n",
    "        )\n",
    "\n",
    "    return eval_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf7d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dl_data_with_spikes(history_offsets, forecast_horizon, nb_detectors, df_base,\n",
    "                                years_split, feature_cols_norm, feature_cols_base,\n",
    "                                weather_lags, spike_config=None):\n",
    "    \"\"\"Extended data prep with optional spike features.\"\"\"\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    df_small = df_base[df_base[\"detector_id\"].isin(df_base[\"detector_id\"].unique()[:nb_detectors])].copy()\n",
    "    df_small = df_small.sort_values([\"detector_id\", \"timestamp\"])\n",
    "    \n",
    "    # Season encoding\n",
    "    df_small.loc[(df_small[\"month\"] <= 2) | (df_small[\"month\"] == 12), \"season\"] = 0\n",
    "    df_small.loc[(df_small[\"month\"] > 2) & (df_small[\"month\"] <= 5), \"season\"] = 1\n",
    "    df_small.loc[(df_small[\"month\"] > 5) & (df_small[\"month\"] <= 8), \"season\"] = 2\n",
    "    df_small.loc[(df_small[\"month\"] > 8) & (df_small[\"month\"] <= 11), \"season\"] = 3\n",
    "    \n",
    "    # Add spike features if configured\n",
    "    feature_cols = feature_cols_base.copy()\n",
    "    feature_cols_norm_full = feature_cols_norm.copy()\n",
    "    \n",
    "    if spike_config is not None:\n",
    "        print(f\"Adding spike features: deltas={spike_config.enable_deltas}, rolling={spike_config.enable_rolling_stats}\")\n",
    "        df_small = add_spike_features(df_small, spike_config)\n",
    "        spike_feature_cols = spike_config.get_feature_columns()\n",
    "        spike_norm_cols = spike_config.get_normalization_columns()\n",
    "        feature_cols = feature_cols + spike_feature_cols\n",
    "        feature_cols_norm_full = feature_cols_norm_full + spike_norm_cols\n",
    "        print(f\"  Added columns: {spike_feature_cols}\")\n",
    "    \n",
    "    # Detector encoding\n",
    "    df_small, det2idx = encode_detectors(df_small)\n",
    "    \n",
    "    # Add weather lag column names\n",
    "    if \"temperature\" in feature_cols:\n",
    "        feature_cols = feature_cols + [f\"temperature_lag_{lag}h\" for lag in weather_lags] \\\n",
    "            + [f\"precipitation_lag_{lag}h\" for lag in weather_lags] \\\n",
    "            + [f\"visibility_lag_{lag}h\" for lag in weather_lags]\n",
    "    \n",
    "    # Split\n",
    "    train = df_small[df_small[\"timestamp\"].dt.year.isin(years_split[0])].copy()\n",
    "    val = df_small[df_small[\"timestamp\"].dt.year.isin(years_split[1])].copy()\n",
    "    test = df_small[df_small[\"timestamp\"].dt.year.isin(years_split[2])].copy() if years_split[2] else None\n",
    "    \n",
    "    train = train.set_index(\"orig_idx\")\n",
    "    val = val.set_index(\"orig_idx\")\n",
    "    if test is not None:\n",
    "        test = test.set_index(\"orig_idx\")\n",
    "    \n",
    "    # Normalization\n",
    "    minmax_cols = [\"lon\", \"lat\", \"year\", \"season\"]\n",
    "    train, val, test, std_scaler, mm_scaler = scale_features(\n",
    "        train, val, test, feature_cols_norm_full, latlon_cols=minmax_cols\n",
    "    )\n",
    "    \n",
    "    # Weather lags\n",
    "    if \"temperature\" in feature_cols_base:\n",
    "        train = add_lags_and_drop(train, weather_lags)\n",
    "        val = add_lags_and_drop(val, weather_lags)\n",
    "        if test is not None:\n",
    "            test = add_lags_and_drop(test, weather_lags)\n",
    "    \n",
    "    # Drop NaNs from spike features\n",
    "    if spike_config is not None:\n",
    "        spike_cols_in_df = [c for c in spike_feature_cols if c in train.columns]\n",
    "        train = train.dropna(subset=spike_cols_in_df)\n",
    "        val = val.dropna(subset=spike_cols_in_df)\n",
    "        if test is not None:\n",
    "            test = test.dropna(subset=spike_cols_in_df)\n",
    "    \n",
    "    # Keep only needed columns (congestion_index is already in feature_cols)\n",
    "    keep_cols = feature_cols + [\"timestamp\", \"detector_id\", \"det_index\"]\n",
    "    keep_cols = [c for c in keep_cols if c in train.columns]\n",
    "    \n",
    "    train = train[keep_cols]\n",
    "    val = val[keep_cols]\n",
    "    if test is not None:\n",
    "        test = test[keep_cols]\n",
    "    \n",
    "    # Build sequences\n",
    "    X_train_hist, Y_train, idx_train, det_train = create_nhits_sequences(\n",
    "        train, feature_cols, history_offsets, forecast_horizon)\n",
    "    X_val_hist, Y_val, idx_val, det_val = create_nhits_sequences(\n",
    "        val, feature_cols, history_offsets, forecast_horizon)\n",
    "    \n",
    "    if test is not None:\n",
    "        X_test_hist, Y_test, idx_test, det_test = create_nhits_sequences(\n",
    "            test, feature_cols, history_offsets, forecast_horizon)\n",
    "    else:\n",
    "        X_test_hist, Y_test, idx_test, det_test = None, None, None, None\n",
    "    \n",
    "    print(f\"Sequences created. Features: {len(feature_cols)}, Train samples: {len(Y_train)}\")\n",
    "    \n",
    "    return (X_train_hist, Y_train, idx_train, det_train,\n",
    "            X_val_hist, Y_val, idx_val, det_val,\n",
    "            X_test_hist, Y_test, idx_test, det_test,\n",
    "            train, val, test, std_scaler, mm_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d99ceb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nb_detectors, forecast_horizon, history_offsets, exp_name, \n",
    "                   df_base, feature_cols_norm, feature_cols, weather_lags, model_config=None,\n",
    "                   years_split=([2019,2020,2021,2022,2023,2024], [2018], [2016]),\n",
    "                   evaluation_years=None, spike_config=None,\n",
    "                   spike_eval_threshold=0.38, criterion=None):\n",
    "    dir = f\"plots_training_dl/{broad_exp_name}/\"\n",
    "\n",
    "    X_train_hist, Y_train, idx_train, det_train, \\\n",
    "    X_val_hist, Y_val, idx_val, det_val, \\\n",
    "    X_test_hist, Y_test, idx_test, det_test, \\\n",
    "    train, val, test, \\\n",
    "    std_scaler, mm_scaler = prepare_dl_data_with_spikes(history_offsets, \n",
    "                                            forecast_horizon, \n",
    "                                            nb_detectors, df_base,\n",
    "                                            feature_cols_norm=feature_cols_norm, \n",
    "                                            feature_cols_base=feature_cols, \n",
    "                                            weather_lags=weather_lags,\n",
    "                                            years_split=years_split,\n",
    "                                            spike_config=spike_config)\n",
    "    \n",
    "    if criterion is None:\n",
    "        criterion = nn.MSELoss()\n",
    "    model = MultiHeadTCNForecaster(**model_config, num_features=X_train_hist.shape[-1])    \n",
    "    params_experiment = {\n",
    "        \"model\": model,\n",
    "        \"optimizer\": torch.optim.Adam(model.parameters(), lr=1e-4),\n",
    "        \"criterion\": criterion,\n",
    "        \"X_train_hist\": X_train_hist,\n",
    "        \"Y_train\": Y_train,\n",
    "        \"train_det_idx\": det_train,\n",
    "        \"X_val_hist\": X_val_hist,\n",
    "        \"Y_val\": Y_val,\n",
    "        \"val_det_idx\": det_val,\n",
    "        \"X_test_hist\": X_test_hist,\n",
    "        \"Y_test\": Y_test,\n",
    "        \"test_det_idx\": det_test,\n",
    "        \"device\": \"cuda\",\n",
    "        \"batch_size\": 512,\n",
    "        \"epochs\": 2,\n",
    "        \"grad_clip\": None,\n",
    "        \"scheduler\": None\n",
    "    }\n",
    "        \n",
    "    if evaluation_years is None:\n",
    "        evaluation_years = years_split[1]\n",
    "    \n",
    "    # RUN EXPERIMENT\n",
    "    model, train_losses, val_losses = run_dl_experiment(**params_experiment, exp_name=exp_name)\n",
    "    eval_df = prepare_eval_df(val, idx_val, predict(model, X_val_hist, det_val), forecast_horizon)\n",
    "    eval_df[\"congestion_index\"] = val.loc[idx_val, \"congestion_index\"].values\n",
    "    evaluate_and_plot_block(eval_df, horizon=forecast_horizon, years=evaluation_years, plot_years=evaluation_years, \n",
    "                            filename=exp_name,\n",
    "                            dir=dir, max_blocks=15,\n",
    "                            eval_spikes=True,\n",
    "                            spike_threshold=spike_eval_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bae83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_csv(FILE_PATH)\n",
    "df_base[\"timestamp\"] = pd.to_datetime(df_base[\"timestamp\"])\n",
    "df_base[\"orig_idx\"] = df_base.index\n",
    "df_base = cyclical_encode(df_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b83017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Adding spike features: deltas=True, rolling=False\n",
      "  Added columns: ['delta_1h', 'delta_2h', 'delta_4h', 'delta_6h']\n",
      "Sequences created. Features: 31, Train samples: 1068551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - training:   0%|          | 0/2087 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'LossConfig' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     29\u001b[39m exp_name = \u001b[33m\"\u001b[39m\u001b[33mtest_pipeline\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m model_config = {\u001b[33m\"\u001b[39m\u001b[33mhorizon\u001b[39m\u001b[33m\"\u001b[39m: forecast_horizon,\n\u001b[32m     32\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mnum_detectors\u001b[39m\u001b[33m\"\u001b[39m: nb_detectors,\n\u001b[32m     33\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m256\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpooling\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m             }\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_detectors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnb_detectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m     \u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m     \u001b[49m\u001b[43mhistory_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mh_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m     \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m     \u001b[49m\u001b[43mdf_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfeature_cols_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_cols_norm_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_cols_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m     \u001b[49m\u001b[43mweather_lags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mw_lags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m     \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m     \u001b[49m\u001b[43myears_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43myears_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m     \u001b[49m\u001b[43mevaluation_years\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2019\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m     \u001b[49m\u001b[43mspike_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspike_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m     \u001b[49m\u001b[43mspike_eval_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_spike_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m     \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(nb_detectors, forecast_horizon, history_offsets, exp_name, df_base, feature_cols_norm, feature_cols, weather_lags, model_config, years_split, evaluation_years, spike_config, spike_eval_threshold, criterion)\u001b[39m\n\u001b[32m     45\u001b[39m     evaluation_years = years_split[\u001b[32m1\u001b[39m]\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# RUN EXPERIMENT\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m model, train_losses, val_losses = \u001b[43mrun_dl_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams_experiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m eval_df = prepare_eval_df(val, idx_val, predict(model, X_val_hist, det_val), forecast_horizon)\n\u001b[32m     50\u001b[39m eval_df[\u001b[33m\"\u001b[39m\u001b[33mcongestion_index\u001b[39m\u001b[33m\"\u001b[39m] = val.loc[idx_val, \u001b[33m\"\u001b[39m\u001b[33mcongestion_index\u001b[39m\u001b[33m\"\u001b[39m].values\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mrun_dl_experiment\u001b[39m\u001b[34m(model, optimizer, criterion, X_train_hist, Y_train, train_det_idx, X_val_hist, Y_val, val_det_idx, X_test_hist, Y_test, test_det_idx, device, batch_size, epochs, grad_clip, scheduler, scaler, exp_name, patience)\u001b[39m\n\u001b[32m     59\u001b[39m model.to(device)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# TRAINING\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m train_losses, val_losses, best_state = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Save losses to file\u001b[39;00m\n\u001b[32m     79\u001b[39m os.makedirs(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mplots_training_dl/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbroad_exp_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adib4\\OneDrive\\Documents\\Projets perso\\CongestionAI\\src\\model_pipelines\\dl_pipeline.py:72\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, scheduler, scaler, device, num_epochs, grad_clip, patience)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Non-AMP branch\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     71\u001b[39m     preds = model(X_batch, det_ids)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     loss.backward()\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m grad_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: 'LossConfig' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fixed parameters\n",
    "feature_cols_norm_base = [\n",
    "    \"temperature\", \"precipitation\", \"visibility\", \"congestion_index\", \"free_flow_speed\"\n",
    "]\n",
    "feature_cols_base = [\n",
    "    \"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\", \"month_sin\", \"month_cos\",\n",
    "    \"lon\", \"lat\", \"year\", \"season\",\n",
    "    \"temperature\", \"precipitation\", \"visibility\",\n",
    "    \"congestion_index\", \"free_flow_speed\"\n",
    "]\n",
    "nb_detectors = 20\n",
    "years_split = [[2016, 2017, 2018, 2020, 2021, 2022, 2023, 2024], [2019], []]\n",
    "\n",
    "# Fixed for now\n",
    "forecast_horizon = 8\n",
    "h_offsets = list(range(24))\n",
    "w_lags = [-1, -2, -6, -8]\n",
    "spike_trigger_threshold = 0.15\n",
    "eval_spike_threshold = 0.38\n",
    "spike_config = SpikeFeatureConfig(\n",
    "        enable_deltas=True,\n",
    "        enable_abs_deltas=False,\n",
    "        enable_rolling_stats=False,\n",
    "        delta_lags=[1, 2, 4, 6],\n",
    ")\n",
    "cfg_loss = LossConfig(loss_type=\"spike_weighted\", spike_weight=3.0, spike_threshold=spike_trigger_threshold)\n",
    "criterion = create_loss(cfg_loss)\n",
    "\n",
    "broad_exp_name = \"multi_head_TCN-best_cfg_gridsearch_spike_features\"\n",
    "exp_name = \"test_pipeline\"\n",
    "\n",
    "model_config = {\"horizon\": forecast_horizon,\n",
    "                \"num_detectors\": nb_detectors,\n",
    "                \"emb_dim\": 256,\n",
    "                \"num_channels\": (128, 256, 256),\n",
    "                \"kernel_size\": 3,\n",
    "                \"dropout\": 0.1,\n",
    "                \"use_se\": False,\n",
    "                \"pooling\": \"last\"\n",
    "            }\n",
    "\n",
    "\n",
    "main(nb_detectors=nb_detectors,\n",
    "     forecast_horizon=forecast_horizon,\n",
    "     history_offsets=h_offsets,\n",
    "     exp_name=exp_name,\n",
    "     df_base=df_base,\n",
    "     feature_cols_norm=feature_cols_norm_base,\n",
    "     feature_cols=feature_cols_base,\n",
    "     weather_lags=w_lags,\n",
    "     model_config=model_config,\n",
    "     years_split=years_split,\n",
    "     evaluation_years=[2019],\n",
    "     spike_config=spike_config,\n",
    "     spike_eval_threshold=eval_spike_threshold,\n",
    "     criterion=criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
