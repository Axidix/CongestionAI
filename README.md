# CongestionAI
AI tool forecasting traffic congestion


get_data_dwd.ipynb - DWD Weather Data Collection
In this notebook, we set up an automated pipeline to retrieve hourly weather data for Berlin using the BrightSky API, selecting the DWD station 00403 (Berlin-Tegel) for consistency across the entire 2015–2025 period. We looped over each day from 2015-01-01 to 2025-01-01, requested the corresponding weather data, and aggregated all responses into a single DataFrame. Only the relevant meteorological variables were kept-temperature, dew point, precipitation, wind speed, wind direction, visibility, cloud cover, and weather conditions. The result is a unified, continuous hourly weather dataset ready to be merged with the traffic data.

get_data_wiz_api.ipynb - Traffic Data Download
This notebook builds a fully automated downloader for the Berlin TEU traffic detector archives. Because the API inconsistently uses two URL patterns depending on the year, we implemented a dual-pattern retrieval system with fallback and year-level flagging for auditing. The script systematically loops through all months from 2015 to 2025, downloads every .csv.gz file, organizes them into annual directories, and confirms consistency across base and alternative URL patterns. The outcome is a complete, structured local archive of all TEU detector data for a 10-year period, without any manual clicking.

clean_traffic_data.ipynb - Traffic Data Cleaning
In this notebook, we loaded the full 10-year traffic dataset, created a clean working copy, and standardized all column names to English. We applied a multi-stage cleaning pipeline: removed duplicates, filtered out low-quality rows based on the provided quality indicator, and discarded physically impossible speed or volume values. We then diagnosed detector reliability, identifying “broken” sensors with constant speeds or massive missing-hour gaps, and excluded them entirely. Additional diagnostics flagged detectors with very high missingness, which were also removed according to our threshold. The end result is a high-quality, reliable dataset of traffic speeds and volumes from functioning detectors, ready for preprocessing and integration with weather and spatial data.

preprocess_traffic_data.ipynb - Traffic Data pre-processing
This notebook prepares the raw Berlin TEU traffic detector dataset for modeling. It loads the cleaned multi-year traffic archive, removes remaining invalid entries, and enriches every observation with meaningful temporal and contextual information. The preprocessing steps include extracting time features (hour, weekday, weekend, month, season, holiday and school-holiday flags), computing detector-specific free-flow speeds, and deriving normalized indicators such as speed ratio and congestion index. The notebook also merges spatial metadata for each detector (location, road name, direction, lane), filters out detectors lacking valid metadata, and ensures consistent formatting across all variables. The final output is a structured, analysis-ready dataset combining high-quality hourly traffic observations with temporal and spatial features.

fix_nans_weather.ipynb -  Weather Data Cleaning and Imputation

This preprocessing step produces a clean and consistent weather dataset for the congestion prediction model. Continuous meteorological variables (temperature, dew point, humidity, visibility, precipitation) are aligned on the timestamp index and interpolated using time-based interpolation to ensure smooth and realistic values. Categorical fields such as `condition` and `icon` are filled using forward and backward filling, as these weather states change gradually over time. Binary indicators (`is_rain`, `is_snow`, `is_fog`) are then recomputed directly from the cleaned continuous variables. Because the original `cloud_cover` column contains long multi-day gaps that cannot be safely interpolated, it is dropped and replaced by a new proxy feature, derived from the weather icon. The result is a fully cleaned, gap-free, and model-ready set of weather features.

