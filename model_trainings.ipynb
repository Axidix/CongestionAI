{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae460a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "\n",
    "df = pd.read_csv('prepared_data/preprocessed_full_data.csv')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28646fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"timestamp\"].dtype)\n",
    "df['timestamp'] = df['timestamp'].astype(str)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "print(df[\"timestamp\"].dtype)\n",
    "\n",
    "# Downscale\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"\n",
    "    Downcast all numeric columns and convert object columns to category.\n",
    "    \"\"\"\n",
    "\n",
    "    df_optimized = df.copy()\n",
    "\n",
    "    # ---- downcast floats ----\n",
    "    float_cols = df_optimized.select_dtypes(include=[\"float64\", \"float32\"]).columns\n",
    "    for col in float_cols:\n",
    "        df_optimized[col] = df_optimized[col].astype(\"float32\")\n",
    "\n",
    "    # ---- downcast ints ----\n",
    "    int_cols = df_optimized.select_dtypes(include=[\"int64\", \"int32\", \"int16\", \"int8\"]).columns\n",
    "    for col in int_cols:\n",
    "        # int64 -> smallest possible int\n",
    "        df_optimized[col] = pd.to_numeric(df_optimized[col], downcast=\"integer\")\n",
    "\n",
    "    # ---- convert strings to categories ----\n",
    "    obj_cols = df_optimized.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in obj_cols:\n",
    "        df_optimized[col] = df_optimized[col].astype(\"category\")\n",
    "\n",
    "    return df_optimized\n",
    "\n",
    "df = optimize_dtypes(df)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_block_predictions(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Ultra-fast evaluation of block forecasts using vectorized operations.\n",
    "    Shape: Y_true, Y_pred = (N, H)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Basic metrics (vectorized) ----\n",
    "    diff = Y_true - Y_pred\n",
    "    mae  = np.mean(np.abs(diff))\n",
    "    rmse = np.sqrt(np.mean(diff * diff))\n",
    "    print(\"MAE:\", mae, \"RMSE:\", rmse)\n",
    "\n",
    "    # ---- Vectorized correlation ----\n",
    "    # Center sequences\n",
    "    Yt = Y_true - Y_true.mean(axis=1, keepdims=True)\n",
    "    Yp = Y_pred - Y_pred.mean(axis=1, keepdims=True)\n",
    "    print(\"Mean centered Y_true and Y_pred for correlation calculation.\")\n",
    "\n",
    "    # Compute numerator: cov\n",
    "    num = np.sum(Yt * Yp, axis=1)\n",
    "\n",
    "    # Denominator: std(True)*std(Pred)\n",
    "    denom = np.sqrt(np.sum(Yt * Yt, axis=1) * np.sum(Yp * Yp, axis=1))\n",
    "    print(\"Computed denominator for correlation calculation.\")\n",
    "\n",
    "    # Avoid division by zero\n",
    "    corr_per_block = np.where(denom == 0, np.nan, num / denom)\n",
    "\n",
    "    # Average correlation (ignoring NaN)\n",
    "    corr = np.nanmean(corr_per_block)\n",
    "    print(\"Correlation:\", corr)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Corr\": corr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_block_predictions(df, horizon=24, detector_id=None,\n",
    "                           years=None, months=None,\n",
    "                           true_prefix=\"future_\", pred_prefix=\"pred_\",\n",
    "                           max_blocks=10):\n",
    "    \"\"\"\n",
    "    Plot 24h forecast trajectories:\n",
    "    - t+1 ... t+horizon for each chosen block\n",
    "    - sample every 'horizon' timestamps\n",
    "    \"\"\"\n",
    "\n",
    "    df_plot = df.copy()\n",
    "\n",
    "    # ---- FILTERING ----\n",
    "    if detector_id is None:\n",
    "        detector_id = df_plot[\"detector_id\"].iloc[0]\n",
    "    df_plot = df_plot[df_plot[\"detector_id\"] == detector_id]\n",
    "\n",
    "    if years is not None:\n",
    "        df_plot = df_plot[df_plot[\"timestamp\"].dt.year.isin(years)]\n",
    "\n",
    "    if months is not None:\n",
    "        df_plot = df_plot[df_plot[\"timestamp\"].dt.month.isin(months)]\n",
    "\n",
    "    # ---- Take blocks every 'horizon' timesteps ----\n",
    "    df_blocks = df_plot.iloc[::horizon].copy()\n",
    "    df_blocks = df_blocks.head(max_blocks)\n",
    "\n",
    "    # ---- Column lists ----\n",
    "    true_cols = [f\"{true_prefix}{h}h\" for h in range(1, horizon+1)]\n",
    "    pred_cols = [f\"{pred_prefix}{h}h\" for h in range(1, horizon+1)]\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    print(\"Plotting...\")\n",
    "    for _, row in df_blocks.iterrows():\n",
    "        base_time = row[\"timestamp\"]\n",
    "        horizon_times = base_time + pd.to_timedelta(np.arange(1, horizon+1), \"h\")\n",
    "\n",
    "        plt.plot(horizon_times, row[true_cols].values,\n",
    "                 label=f\"True (start {base_time})\", alpha=0.6)\n",
    "\n",
    "        plt.plot(horizon_times, row[pred_cols].values,\n",
    "                 label=f\"Pred (start {base_time})\", alpha=0.6)\n",
    "\n",
    "    plt.title(f\"{horizon}-hour Forecast Trajectories\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Congestion Index\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c010f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_block(df, horizon=24,\n",
    "                            detector_id=None,\n",
    "                            years=None,\n",
    "                            months=None,\n",
    "                            true_prefix=\"future_\",\n",
    "                            pred_prefix=\"pred_\"):\n",
    "    \"\"\"\n",
    "    Full multi-step forecast evaluation and plotting.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Extract arrays ----\n",
    "    true_cols = [f\"{true_prefix}{h}h\" for h in range(1, horizon+1)]\n",
    "    pred_cols = [f\"{pred_prefix}{h}h\" for h in range(1, horizon+1)]\n",
    "\n",
    "    Y_true = df[true_cols].values\n",
    "    Y_pred = df[pred_cols].values\n",
    "\n",
    "    # ---- Compute metrics ----\n",
    "    metrics = evaluate_block_predictions(Y_true, Y_pred)\n",
    "\n",
    "    print(\"=== Block Forecast Evaluation ===\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    # ---- Plot ----\n",
    "    plot_block_predictions(\n",
    "        df,\n",
    "        horizon=horizon,\n",
    "        detector_id=detector_id,\n",
    "        years=years,\n",
    "        months=months,\n",
    "        true_prefix=true_prefix,\n",
    "        pred_prefix=pred_prefix\n",
    "    )\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_baseline_multi(df, window_size=5, horizon=24):\n",
    "    df_h = df[[\"detector_id\", \"timestamp\", \"congestion_index\"]].copy()\n",
    "\n",
    "    # For each horizon h = 1..24 create a future target\n",
    "    for h in range(1, horizon+1):\n",
    "        df_h[f\"future_{h}h\"] = (\n",
    "            df_h.groupby(\"detector_id\")[\"congestion_index\"]\n",
    "                .shift(-h)\n",
    "        )\n",
    "\n",
    "    # Baseline uses rolling mean of past\n",
    "    df_h[\"hist_baseline\"] = (\n",
    "        df_h.groupby(\"detector_id\")[\"congestion_index\"]\n",
    "             .rolling(window_size, min_periods=1)\n",
    "             .mean()\n",
    "             .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Expand baseline into 24 identical horizons\n",
    "    for h in range(1, horizon+1):\n",
    "        df_h[f\"pred_{h}h\"] = df_h[\"hist_baseline\"]\n",
    "\n",
    "    # Drop rows where ANY future target is missing\n",
    "    future_cols = [f\"future_{h}h\" for h in range(1, horizon+1)]\n",
    "    df_h = df_h.dropna(subset=future_cols)\n",
    "\n",
    "    return df_h\n",
    "\n",
    "\n",
    "print(\"Baseline: historical average congestion.\")\n",
    "df_historical = historical_baseline_multi(df, horizon=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_plot_block(df_historical, horizon=24, years=[2019])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lags function\n",
    "def add_lag_features(df, lags: list):\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}h'] = (\n",
    "            df.groupby(\"detector_id\")[\"congestion_index\"]\n",
    "                      .shift(lag)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Adds future targets\n",
    "def add_future_targets(df, horizon: int):\n",
    "    for h in range(1, horizon + 1):\n",
    "        df[f'future_{h}h'] = (\n",
    "            df.groupby(\"detector_id\")[\"congestion_index\"]\n",
    "                      .shift(-h)\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for trainings (LGMB...)\n",
    "\n",
    "horizon = 24\n",
    "lags = [1, 6, 12, 24]\n",
    "\n",
    "feature_cols = [\n",
    "    # --- Time features ---\n",
    "    \"hour\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"year\",              # optional, can remove to avoid overfitting\n",
    "    #\"is_weekend\",\n",
    "\n",
    "    # --- Calendar features ---\n",
    "    \"is_holiday\",\n",
    "    \"is_school_holiday\",\n",
    "    \"is_rush_hour\",\n",
    "    #\"season\",            # categorical\n",
    "\n",
    "    # --- Weather features ---\n",
    "    \"temperature\",\n",
    "    #\"dew_point\",\n",
    "    \"precipitation\",\n",
    "    #\"relative_humidity\",\n",
    "    \"visibility\",\n",
    "    \"cloud_cover\",\n",
    "    #\"is_rain\",\n",
    "    #\"is_snow\",\n",
    "    #\"is_fog\",\n",
    "\n",
    "    # --- Road info ---\n",
    "    \"free_flow_speed\",   # numerical\n",
    "    \"detector_id\",      # categorical\n",
    "    \"congestion_index\",  # current value\n",
    "\n",
    "    # --- Spatial features ---\n",
    "    \"lon\",\n",
    "    \"lat\"\n",
    "]\n",
    "\n",
    "lags_cols = [f'lag_{lag}h' for lag in lags]\n",
    "target_cols = [f'future_{h}h' for h in range(1, horizon + 1)]\n",
    "\n",
    "\n",
    "df[\"detector_id\"] = df[\"detector_id\"].astype(\"category\")\n",
    "if \"season\" in df.columns: \n",
    "    df[\"season\"] = df[\"season\"].astype(\"category\")\n",
    "df = add_lag_features(df, lags)\n",
    "df = add_future_targets(df, horizon)\n",
    "\n",
    "df = df.dropna(subset=lags_cols + target_cols)\n",
    "df.drop(columns=df.columns.difference(feature_cols + lags_cols + target_cols), inplace=True)\n",
    "\n",
    "for col in df.select_dtypes(include=[\"float64\"]).columns:\n",
    "    df[col] = df[col].astype(\"float32\")\n",
    "\n",
    "for col in df.select_dtypes(include=[\"int64\"]).columns:\n",
    "    df[col] = df[col].astype(\"int32\")\n",
    "\n",
    "years_train = [2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "years_val = [2022, 2023]\n",
    "years_test = [2024]\n",
    "\n",
    "train_df = df[df[\"year\"].isin([2015, 2016, 2017, 2018, 2019, 2020, 2021])]\n",
    "val_df   = df[df[\"year\"].isin([2022, 2023])]\n",
    "test_df  = df[df[\"year\"].isin([2024])]\n",
    "\n",
    "X_train = train_df[feature_cols + lags_cols]\n",
    "X_val   = val_df[feature_cols + lags_cols]\n",
    "X_test  = test_df[feature_cols + lags_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b809dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGMB Model training\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "models = {}\n",
    "horizon = 24\n",
    "\n",
    "# LightGBM params\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"l2\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 96,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 3,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "    \"max_bin\": 255,\n",
    "    \"verbosity\": -1,\n",
    "    \n",
    "    # GPU SUPPORT\n",
    "    \"device\": \"cpu\",\n",
    "    \"gpu_platform_id\": -1,\n",
    "    \"gpu_device_id\": -1,\n",
    "    \"gpu_use_dp\": False\n",
    "}\n",
    "\n",
    "categorical_features = [\"detector_id\", \"season\"]\n",
    "\n",
    "print(\"Training 24 LightGBM models...\")\n",
    "\n",
    "for i, tgt in enumerate(target_cols):\n",
    "    print(f\"\\n - Training horizon {i+1}/{horizon}: {tgt}\")\n",
    "\n",
    "    y_train = train_df[tgt].values\n",
    "    y_val   = val_df[tgt].values\n",
    "\n",
    "    train_ds = lgb.Dataset(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        categorical_feature=categorical_features\n",
    "    )\n",
    "\n",
    "    val_ds = lgb.Dataset(\n",
    "        X_val, \n",
    "        y_val,\n",
    "        categorical_feature=categorical_features,\n",
    "        reference=train_ds\n",
    "    )\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_ds,\n",
    "        valid_sets=[train_ds, val_ds],\n",
    "        valid_names=[\"train\", \"val\"],\n",
    "        num_boost_round=2000\n",
    "    )\n",
    "\n",
    "    models[tgt] = model\n",
    "\n",
    "print(\"\\n Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcfa8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Fast LightGBM Training (Optimized for Speed)\n",
    "# =====================================================\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "horizon = 24\n",
    "models = {}\n",
    "\n",
    "# ================================\n",
    "# 1) FAST DATA REDUCTION\n",
    "# ================================\n",
    "print(\"Sampling training data for speed...\")\n",
    "train_df_sampled = train_df.sample(frac=0.30, random_state=42)\n",
    "\n",
    "X_train = train_df_sampled[feature_cols + lags_cols]\n",
    "X_val   = val_df[feature_cols + lags_cols]\n",
    "\n",
    "# ================================\n",
    "# 2) LightGBM FAST CPU PARAMS\n",
    "# ================================\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"l2\",\n",
    "    \"learning_rate\": 0.05,\n",
    "\n",
    "    # Faster trees\n",
    "    \"num_leaves\": 48,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"max_bin\": 255,\n",
    "\n",
    "    # Regularization to stabilize\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 3,\n",
    "\n",
    "    \"verbosity\": -1,\n",
    "\n",
    "    # CPU FAST MODE\n",
    "    \"device\": \"cpu\",\n",
    "    \"num_threads\": -1\n",
    "}\n",
    "\n",
    "categorical_features = [\"detector_id\", \"season\"] if \"season\" in df.columns else [\"detector_id\"]\n",
    "target_cols = [f\"future_{h}h\" for h in range(1, horizon+1)]\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 3) TRAIN ONE HORIZON\n",
    "# ================================\n",
    "def train_one_horizon(i, tgt):\n",
    "    print(f\"\\n - Training horizon {i}/{horizon}: {tgt}\")\n",
    "\n",
    "    train_ds = lgb.Dataset(\n",
    "        X_train,\n",
    "        train_df_sampled[tgt].values,\n",
    "        categorical_feature=categorical_features\n",
    "    )\n",
    "\n",
    "    val_ds = lgb.Dataset(\n",
    "        X_val,\n",
    "        val_df[tgt].values,\n",
    "        categorical_feature=categorical_features,\n",
    "        reference=train_ds\n",
    "    )\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_ds,\n",
    "        valid_sets=[val_ds],\n",
    "        valid_names=[\"val\"],\n",
    "        num_boost_round=800,               # much smaller\n",
    "    )\n",
    "\n",
    "    return tgt, model\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 4) PARALLEL TRAINING (BIG SPEEDUP)\n",
    "# ================================\n",
    "print(\"Training 24 LightGBM models in parallel...\")\n",
    "results = Parallel(n_jobs=2)(   # adjust based on CPU cores\n",
    "    delayed(train_one_horizon)(i+1, tgt)\n",
    "    for i, tgt in enumerate(target_cols)\n",
    ")\n",
    "\n",
    "# Store results\n",
    "models = {tgt: model for tgt, model in results}\n",
    "\n",
    "print(\"\\n Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_24h(models, X):\n",
    "    preds = np.zeros((len(X), horizon), dtype=\"float32\")\n",
    "    for i, tgt in enumerate(target_cols):\n",
    "        preds[:, i] = models[tgt].predict(X, num_iteration=models[tgt].best_iteration)\n",
    "    return preds\n",
    "\n",
    "\n",
    "Y_pred_test = predict_24h(models, X_test)\n",
    "\n",
    "# Convert to DataFrame\n",
    "pred_cols = [f\"pred_{h}h\" for h in range(1, horizon+1)]\n",
    "pred_df = pd.DataFrame(Y_pred_test, columns=pred_cols)\n",
    "pred_df[\"timestamp\"] = test_df[\"timestamp\"].values\n",
    "pred_df[\"detector_id\"] = test_df[\"detector_id\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = test_df.reset_index(drop=True).join(pred_df[pred_cols])\n",
    "results = evaluate_block_predictions(\n",
    "    merged[target_cols].values,\n",
    "    merged[pred_cols].values\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
