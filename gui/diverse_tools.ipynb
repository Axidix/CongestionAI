{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f6aade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting osmnx\n",
      "  Obtaining dependency information for osmnx from https://files.pythonhosted.org/packages/0e/a7/8606797abfd9a47cd11f26ca7d70b818d9e2f5346811d797efb3429b7603/osmnx-2.0.7-py3-none-any.whl.metadata\n",
      "  Downloading osmnx-2.0.7-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting geopandas>=1.0.1 (from osmnx)\n",
      "  Obtaining dependency information for geopandas>=1.0.1 from https://files.pythonhosted.org/packages/0b/70/d5cd0696eff08e62fdbdebe5b46527facb4e7220eabe0ac6225efab50168/geopandas-1.1.1-py3-none-any.whl.metadata\n",
      "  Downloading geopandas-1.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: networkx>=2.5 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from osmnx) (3.5)\n",
      "Requirement already satisfied: numpy>=1.22 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from osmnx) (2.3.3)\n",
      "Requirement already satisfied: pandas>=1.4 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from osmnx) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.27 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from osmnx) (2.32.5)\n",
      "Collecting shapely>=2.0 (from osmnx)\n",
      "  Obtaining dependency information for shapely>=2.0 from https://files.pythonhosted.org/packages/13/02/58b0b8d9c17c93ab6340edd8b7308c0c5a5b81f94ce65705819b7416dba5/shapely-2.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading shapely-2.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas>=1.0.1->osmnx)\n",
      "  Obtaining dependency information for pyogrio>=0.7.2 from https://files.pythonhosted.org/packages/89/a4/0aef5837b4e11840f501e48e01c31242838476c4f4aff9c05e228a083982/pyogrio-0.12.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading pyogrio-0.12.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: packaging in /Data/CongestionAI/venv/lib/python3.11/site-packages (from geopandas>=1.0.1->osmnx) (25.0)\n",
      "Collecting pyproj>=3.5.0 (from geopandas>=1.0.1->osmnx)\n",
      "  Obtaining dependency information for pyproj>=3.5.0 from https://files.pythonhosted.org/packages/ad/ab/9bdb4a6216b712a1f9aab1c0fcbee5d3726f34a366f29c3e8c08a78d6b70/pyproj-3.7.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading pyproj-3.7.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from pandas>=1.4->osmnx) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from pandas>=1.4->osmnx) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from pandas>=1.4->osmnx) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from requests>=2.27->osmnx) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from requests>=2.27->osmnx) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from requests>=2.27->osmnx) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from requests>=2.27->osmnx) (2025.11.12)\n",
      "Requirement already satisfied: six>=1.5 in /Data/CongestionAI/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.4->osmnx) (1.17.0)\n",
      "Downloading osmnx-2.0.7-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading geopandas-1.1.1-py3-none-any.whl (338 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m338.4/338.4 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyogrio-0.12.1-cp311-cp311-manylinux_2_28_x86_64.whl (32.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.5/32.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyproj-3.7.2-cp311-cp311-manylinux_2_28_x86_64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shapely, pyproj, pyogrio, geopandas, osmnx\n",
      "Successfully installed geopandas-1.1.1 osmnx-2.0.7 pyogrio-0.12.1 pyproj-3.7.2 shapely-2.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install osmnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f288b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Berlin road network\n",
    "import osmnx as ox\n",
    "\n",
    "G = ox.graph_from_place(\"Berlin, Germany\", network_type=\"drive\")\n",
    "edges = ox.graph_to_gdfs(G, nodes=False)\n",
    "\n",
    "# Create unique road ID from MultiIndex\n",
    "edges[\"road_id\"] = edges.index.map(lambda idx: f\"{idx[0]}_{idx[1]}_{idx[2]}\")\n",
    "\n",
    "# Save\n",
    "edges.to_file(\"data/berlin_roads.geojson\", driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac09078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/CongestionAI/venv/lib/python3.11/site-packages/geopandas/io/file.py:576: UserWarning: Could not parse column 'reversed' as JSON; leaving as string\n",
      "  return pyogrio.read_dataframe(path_or_bytes, bbox=bbox, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated forecast for 73687 roads\n"
     ]
    }
   ],
   "source": [
    "# Create dummy forecast data\n",
    "\n",
    "import json\n",
    "import random\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load the roads we just created\n",
    "roads = gpd.read_file(\"data/berlin_roads.geojson\")\n",
    "\n",
    "# Get all road IDs\n",
    "road_ids = roads[\"road_id\"].tolist()\n",
    "\n",
    "# Generate dummy forecast data (24 hours of congestion values between 0 and 1)\n",
    "forecast_data = {\n",
    "    road_id: [round(random.random(), 2) for _ in range(25)]\n",
    "    for road_id in road_ids\n",
    "}\n",
    "\n",
    "# Create forecast JSON\n",
    "forecast = {\n",
    "    \"generated_at\": \"2025-01-01T12:00:00Z\",\n",
    "    \"data\": forecast_data\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open(\"data/forecast.json\", \"w\") as f:\n",
    "    json.dump(forecast, f, indent=2)\n",
    "\n",
    "print(f\"Generated forecast for {len(road_ids)} roads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6545ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute and save the road network graph for routing\n",
    "import osmnx as ox\n",
    "\n",
    "# Download Berlin drive graph (heavy operation)\n",
    "G = ox.graph_from_place(\"Berlin, Germany\", network_type=\"drive\")\n",
    "\n",
    "# Save graph to file\n",
    "ox.save_graphml(G, \"data/berlin_drive.graphml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80b6644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data (only needed columns)...\n",
      "Found 380 unique detectors\n",
      "Loading WFS segments...\n",
      "Loaded 18135 road segments\n",
      "Finding nearest segment for each detector...\n",
      "Dropped 0 detectors > 300m from any segment\n",
      "\n",
      "Saved mapping: /Data/CongestionAI/backend/data/detector_to_wfs_segment.parquet\n",
      "Matched: 580 / 580 detectors\n",
      "       detector_id                      unique_id     dist_m        lon  \\\n",
      "0  100101010000369  32450044_32460012.01_32450044   1.532453  13.192747   \n",
      "1  100101010000874  36450013_36460004.01_36450013   4.307244  13.261301   \n",
      "2  100101010000975  36450013_36460004.01_36450013   4.307244  13.261301   \n",
      "3  100101010001076  36450014_36450015.01_36450014   2.004016  13.263105   \n",
      "3  100101010001076  36450014_36450015.01_36450015   2.004016  13.263105   \n",
      "4  100101010001177  36450014_36450015.01_36450014   2.004016  13.263105   \n",
      "4  100101010001177  36450014_36450015.01_36450015   2.004016  13.263105   \n",
      "5  100101010001379  36450013_36450040.01_36450013   2.220923  13.259881   \n",
      "6  100101010001884  44450022_44450023.02_44450022  11.465399  13.379267   \n",
      "6  100101010001884  44450022_44450023.02_44450023  11.465399  13.379267   \n",
      "\n",
      "         lat  \n",
      "0  52.433810  \n",
      "1  52.436640  \n",
      "2  52.436640  \n",
      "3  52.435112  \n",
      "3  52.435112  \n",
      "4  52.435112  \n",
      "4  52.435112  \n",
      "5  52.433743  \n",
      "6  52.436142  \n",
      "6  52.436142  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "\n",
    "# ---- Inputs ----\n",
    "PREPROCESSED_DATA_PATH = \"/Data/CongestionAI/prepared_data/preprocessed_full_data.csv\"\n",
    "WFS_JSON_PATH = \"/Data/CongestionAI/json_api_traffic_test.json\"\n",
    "OUT_PATH = \"/Data/CongestionAI/backend/data/detector_to_wfs_segment.parquet\"\n",
    "\n",
    "# ---- Extract unique detectors from preprocessed data ----\n",
    "print(\"Loading preprocessed data (only needed columns)...\")\n",
    "det = pd.read_csv(\n",
    "    PREPROCESSED_DATA_PATH, \n",
    "    usecols=[\"detector_id\", \"lon\", \"lat\"]\n",
    ").drop_duplicates(subset=[\"detector_id\"])\n",
    "print(f\"Found {len(det)} unique detectors\")\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "g_det = gpd.GeoDataFrame(\n",
    "    det,\n",
    "    geometry=gpd.points_from_xy(det[\"lon\"], det[\"lat\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "# ---- Load WFS segments from JSON (it's actually valid GeoJSON) ----\n",
    "print(\"Loading WFS segments...\")\n",
    "g_seg = gpd.read_file(WFS_JSON_PATH)\n",
    "print(f\"Loaded {len(g_seg)} road segments\")\n",
    "\n",
    "# Ensure the id column exists\n",
    "if \"unique_id\" not in g_seg.columns:\n",
    "    raise ValueError(\"WFS segments missing 'unique_id' in properties\")\n",
    "\n",
    "# ---- Work in metric CRS for correct distances ----\n",
    "# Berlin: EPSG:25833 is a good metric CRS\n",
    "g_det_m = g_det.to_crs(\"EPSG:25833\")\n",
    "g_seg_m = g_seg.to_crs(\"EPSG:25833\")[[\"unique_id\", \"geometry\"]]\n",
    "\n",
    "# ---- Nearest segment per detector ----\n",
    "print(\"Finding nearest segment for each detector...\")\n",
    "joined = gpd.sjoin_nearest(\n",
    "    g_det_m[[\"detector_id\", \"lon\", \"lat\", \"geometry\"]],\n",
    "    g_seg_m,\n",
    "    how=\"left\",\n",
    "    distance_col=\"dist_m\",\n",
    ")\n",
    "\n",
    "# Optional: drop matches too far away (tune threshold)\n",
    "MAX_DIST_M = 300\n",
    "n_far = (joined[\"dist_m\"] > MAX_DIST_M).sum()\n",
    "joined.loc[joined[\"dist_m\"] > MAX_DIST_M, \"unique_id\"] = pd.NA\n",
    "print(f\"Dropped {n_far} detectors > {MAX_DIST_M}m from any segment\")\n",
    "\n",
    "mapping = joined[[\"detector_id\", \"unique_id\", \"dist_m\", \"lon\", \"lat\"]].copy()\n",
    "\n",
    "# Save\n",
    "mapping.to_parquet(OUT_PATH, index=False)\n",
    "print(f\"\\nSaved mapping: {OUT_PATH}\")\n",
    "print(f\"Matched: {mapping['unique_id'].notna().sum()} / {len(mapping)} detectors\")\n",
    "print(mapping.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
