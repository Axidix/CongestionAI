First 3 experiments (MLP forecaster, with shceduler or not, MSE or SmoothL1Loss):
- overfitting since epoch 1; train loss always decreasing but val increasing
- stagnates around 0.45 correlation - 0.5 rmse
- model probably too big (too many params for an MLP)
- MLP --> no learning sequences / temporal info
- not enough regularization ?


Experiment : 4 models (transformer, MLP, TCN, Nhits), simpler MLP with more dropout, default params
--> Same issue, let's try to see if it actually overfits on train, using smaller horizon

1h horizon, 1 detector: no problem, even if preds are not perfect
1h horizon, 200 detectors: huge val loss for TCN and MLP (increasing), stagnating but low for Nhits and transformer
==> I still don't understand the problem

Let's keep it simple: we will use onky N-hits, with fewer (one or a few) years, less features, until we find the root cause

With a high embeddding dimensiont (256vs32), the detector problem seems solved: even with 200, no issue spotted.
Still the problem of forecast horizon: bad val loss even with 4.

More complex models + lower learning rate + Adam optim, training on 20 detectors and 1,4,8,14,24h horizon:
- loss plateau-ing after several epochs, but at low value + approx. follow train curve at the beginning
--> seems like no inherent pipeline issue now, just a model not good enough
--> We can go forward:
    - Scale data? add back years, features, expand history/lags...
    - Use more suitable model? (e.g multi head regression? transformer?)
    - Use a 1-step model ?

Let's keep an 8 hour forecast for now and scale data:
    - add back features
    - add more years to train / select correctly train/val/test
    - add more history/lags when suitable
    - add more detectors
    - if still correct, go on model tuning
For better analysis, add per-step metrics.

VERY VERY IMPORTANT:
loss at 20 detectors is way lower than at 1: more data? issue in the model? useless emb?

Test features relevance:
- no obvious differences between any setup
- train loss a bit lower with full features (with hand-crafted or not)
- AI analysis: "The differences are relatively small (~8% between best and worst), suggesting the model relies heavily on temporal patterns in congestion_index itself. Weather and context features provide incremental improvements."

Test features relevance:
- no obvious differences except 1 week dense history: loss ten times higher, but curves have similar shape
- 4 days dense seems a good in between: low train/val loss, quite a lot of history for future experiments
- even half a day seems enough, which is really weird: maybe data is so repetitive that it learns what is going on the past day at the same hour*
- best metrics for 1 day hourly history

REMINDER: these metrics are computed on train dataset.
We will now switch back to test dataset to actually evaluate the best generalizator
We also use val year=2019 to not have an "extreme" year as the validation data

Grid search on history x weather_lags x model_depth:
Across all configurations, the grid-search results show a consistent pattern: short-term, high-resolution history (24h hourly) and sparse weather lags provide the strongest predictive signal with the lowest and most stable validation losses, while longer histories or dense lag structures offer no benefit and tend to increase overfitting. Model depth and width have only a marginal impact, indicating that capacity is not currently the limiting factor; instead, performance is driven primarily by feature design and temporal context. Sparse lag features and compact models also provide the best trade-off between accuracy and efficiency, confirming that the problem benefits from concise, recent inputs rather than broader historical or high-dimensional weather information.

NEW STEPS
The general shapes are okay, but forecast on spikes is bad. Let's try to work on that. Two axes:
- crafted features for spikes/deltas/rolling averages
- personalized losses weighting spikes/great deltas more

The current models are playing it safe. They have learned that predicting "no spike" is usually right (since spikes are ~35% of data), so it:
    - Predicts spikes conservatively → high precision, terrible recall
    - Essentially smooths toward the mean → low delta correlation
    - Misses most actual congestion events → the exact thing you want to predict
(insights from new spike metrics)

We will now proceed to a new grid search on adding crafted spikes features + personalized losses

The experiments show that the best-performing configuration is deltas_only features combined with the spike_weighted loss, which simultaneously achieves the lowest RMSE and highest spike F1. In contrast, using all spike features (full_spike) tends to degrade performance unless paired with a specialized loss, and MSE consistently underperforms on spike-related metrics. Overall, the model benefits most from lightweight spike features (deltas) and a loss function designed to emphasize rare events, confirming that careful feature selection and spike-aware training objectives are crucial for balancing accuracy and spike detection.
Full analysis (values+plots) can be found in the dedicated folder

One issue to precise here: spike threshold is too low in the grid search (0.15).
We will study the threshold to choose: 
    Delta stats (z-normalized):
    Mean: 0.1639
    Std:  0.1985
    Median: 0.1011
    90th percentile: 0.3797
    95th percentile: 0.5263
    99th percentile: 0.9592

    % flagged as spike with threshold=0.15: 36.4%
    Recommended threshold (90th pct): 0.380

Let's start working on a more advanced model: TCN multi-head regressor
The multi-head TCN is clearly good at smooth trajectory forecasting: it gives very competitive MAE/RMSE compared to NHITS, tracks the overall shape and timing of congestion reasonably well over 8 steps, and trains in a stable, non-overfitting way with nicely decreasing train/val losses. It’s especially solid on the first ~4–5 horizons, where errors stay low and correlation with the true signal is high.

Let's scale to a 24hours horizon.
Test training with same parameters as 8h horizon: The model remained stable, showed no overfitting, and achieved strong long-range correlation (0.61) with only a modest increase in MAE/RMSE. Short-term horizons (1–8) matched the 8-hour baseline, and errors increased smoothly rather than collapsing for later steps. Forecasts are coherent but overly smoothed, with peak amplitudes and spike events largely underestimated. Overall, this provides a solid long-horizon baseline and clearly indicates the next priorities: larger receptive fields (more depth or bigger kernels), attention pooling, and longer training to better capture high-frequency dynamics.

Test of 14 configs over model depth, kernel_size, attention, SE:
The architectural sweep highlighted that the TCN’s performance at 24-hour forecasting is primarily limited by its receptive field. Increasing the kernel size to 7 and adding a fourth TCN block consistently improved validation loss, with the best configuration (4 layers, kernel size 7) outperforming the baseline by roughly 1.3%. Width increases had smaller effects, and neither attention pooling nor SE blocks provided benefits at this stage. These results clearly show that expanding temporal context is the most effective direction before introducing more complex lag or weather features.

Test of more history and weather lags:
The best results were obtained with a sparse 24-hour weather lag strategy (lags at 0, 3, 6, 12, and 24 hours). This configuration outperformed all others in validation loss and improved overall trend tracking without increasing feature noise. Extending congestion history to 48 hours improved correlation slightly but offered no gain in error metrics. Dense weather lags up to 24 hours performed significantly worse, confirming that sparse long-range weather memory is more informative than dense, highly correlated lag inputs.
48h history config will be kept for now.

Reintroduction of binary features: seems quite useful, let's keep them.
Creation of rolling volatility features: small gains in RMSE, correlation and spike precision, let's keep them.

A batch of 15 experiments was run to test variations in spike thresholds, delta-lag configurations, loss formulations, TCN depth, kernel size, pooling strategies, and extended history/weather features. Because several experiments used different loss types or spike weights, validation losses were not comparable across the entire batch; evaluation relied only on model-agnostic metrics (MAE, RMSE, correlations, spike recall/precision/F1, and direction metrics).
The main gains were associated with architectural changes. Increasing the temporal receptive field (36-hour history, wider kernels) and adding deeper TCN stacks produced consistent improvements in both forecasting accuracy and spike-related metrics. Dense weather lags and volatility features contributed additional incremental improvements. Average pooling performed better than last-timestep pooling for spike detection. Loss tuning with lower spike weights (≈2) showed potential when evaluated through final metrics only.
Overall, improvements were mainly linked to expanded temporal context and receptive-field adjustments rather than loss-type changes. These directions are the most promising for scaling toward a larger multi-detector model.

Test scheduling:
Four LR schedulers were tested with lr=1e-4 (none, Cosine, Cosine Warm Restarts, One-Cycle) while keeping architecture, features, and loss constant. Cosine Annealing produced the best validation loss (0.1124), the best correlation (0.6327), and the most stable spike metrics. Warm Restarts and One-Cycle were less stable and showed no advantage on this short training regime. Cosine also reduced overfitting compared to no scheduler. Conclusion: Cosine Annealing LR is now the recommended default training schedule.

Test optimizers:
Optimizer experiments showed that forecasting performance varies only minimally across optimization algorithms. AdamW and Adam with cosine annealing perform similarly, with the best overall configuration being AdamW (lr=3e-4, β₂=0.99). Spike detection metrics remain largely unchanged across optimizers, confirming that the optimizer is not the performance bottleneck at this stage. The model has reached an optimizer-insensitive plateau, meaning further improvements should focus on regularization, feature engineering, and training dynamics rather than optimizer changes.

Test regularization:
Tested a range of regularization configurations (dropout, weight decay, gradient clipping, reduced embedding dimension). Results show the model is robust to most regularization changes. Aggressive regularization (drop0.25 + wd1e-3 + clip0.5) gives the best global correlation and spike-direction accuracy, while a smaller model yields the best RMSE and spike precision. No configuration meaningfully improves spike recall or F1, indicating that model capacity and regularization are not the bottleneck for spikes. Future work should focus on feature engineering, spike-specific loss weighting, and data balancing.

Scaling to 100 detectors:
Great loss in performance: train/val loss doubled, MAE higher... Model needs more depth? more features?

Optimization of training:
Use of torch.compile, amp scaler, larger batch size, workers to reduce training time --> 6min per epoch to 2.5min
Performances don't seem to be affected consequently. However it still means about 1h per training.
We might as well try a transfomer models, despite iterating on TCN for a long time.

Ran a controlled comparison between our best TCN and several Transformer architectures with longer history (48–96h), on 20 detectors. The 24h TCN (≈1.9M params) still performs extremely well (val loss ≈ 0.111, spike F1 ≈ 0.48). Medium-size Transformers with 72–96h history can slightly improve validation loss and spike MAE (best config: 72h Transformer with “last” pooling), but overall spike F1 and directional metrics remain very close to the TCN, while training time increases by a factor ~6–8 and models become harder to train. A very large Transformer (≈6.5M params) yields marginal F1 gains but worse val loss and heavy compute. 
Conclusion: keep the Transformer as a high-performance offline / demo model, and focus on a lighter TCN (24h history) as the main deployable model, possibly further compressed (smaller emb_dim / channels) with minimal loss in accuracy.

We will now stop experiments that increase complexity: we aime for a lighter version of the model that keeps good accuracy.



