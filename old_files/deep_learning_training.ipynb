{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adbacbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183cb038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f2ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading only required columns...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23385313 entries, 0 to 23385312\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Dtype         \n",
      "---  ------             -----         \n",
      " 0   timestamp          datetime64[ns]\n",
      " 1   detector_id        category      \n",
      " 2   hour               int32         \n",
      " 3   day_of_week        int32         \n",
      " 4   is_weekend         int32         \n",
      " 5   month              int32         \n",
      " 6   is_holiday         int32         \n",
      " 7   is_rush_hour       int32         \n",
      " 8   is_school_holiday  int32         \n",
      " 9   congestion_index   float32       \n",
      " 10  temperature        float32       \n",
      " 11  precipitation      float32       \n",
      " 12  visibility         float32       \n",
      " 13  is_snow            int32         \n",
      " 14  is_fog             int32         \n",
      "dtypes: category(1), datetime64[ns](1), float32(4), int32(9)\n",
      "memory usage: 1.4 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Path to your cleaned giant file\n",
    "FILE_PATH = \"prepared_data/preprocessed_full_data.csv\"\n",
    "\n",
    "# Columns we actually need for deep learning\n",
    "keep_cols = [\n",
    "    \"timestamp\", \"detector_id\", \"congestion_index\",\n",
    "    \"hour\", \"day_of_week\", \"month\", \"is_weekend\",\n",
    "    \"is_holiday\", \"is_school_holiday\", \"is_rush_hour\",\n",
    "    \"temperature\", \"precipitation\", \"visibility\", \"is_snow\", \"is_fog\"\n",
    "]\n",
    "\n",
    "print(\"Loading only required columns...\")\n",
    "df = pd.read_csv(FILE_PATH, usecols=keep_cols)\n",
    "\n",
    "# Memory reduction\n",
    "for col in df.select_dtypes(\"float64\"):\n",
    "    df[col] = df[col].astype(\"float32\")\n",
    "\n",
    "for col in df.select_dtypes(\"int64\"):\n",
    "    df[col] = df[col].astype(\"int32\")\n",
    "\n",
    "#df[\"detector_id\"] = df[\"detector_id\"].astype(\"category\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ad30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical encoding of time features\n",
    "df[\"hour_sin\"]  = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "df[\"hour_cos\"]  = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "\n",
    "df[\"dow_sin\"]   = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "df[\"dow_cos\"]   = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "\n",
    "df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small dataframe size: (40794, 21)\n"
     ]
    }
   ],
   "source": [
    "# Pick a few detectors for fast experimenting\n",
    "#sample_detectors = df[\"detector_id\"].cat.categories[:3]  # first n detectors\n",
    "sample_detectors = df[\"detector_id\"].unique()[:3]  # first n detectors\n",
    "\n",
    "df_small = df[df[\"detector_id\"].isin(sample_detectors)]\n",
    "#df_small[\"detector_id\"] = df_small[\"detector_id\"].cat.remove_unused_categories()\n",
    "\n",
    "\n",
    "# Restrict to 2019-2020 for prototyping\n",
    "df_small = df_small[df_small[\"timestamp\"].dt.year.isin([2022, 2020, 2021])]\n",
    "\n",
    "df_small = df_small.sort_values([\"detector_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Small dataframe size:\", df_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219ac526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>detector_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>month</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_rush_hour</th>\n",
       "      <th>is_school_holiday</th>\n",
       "      <th>congestion_index</th>\n",
       "      <th>...</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>visibility</th>\n",
       "      <th>is_snow</th>\n",
       "      <th>is_fog</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>-1792767705</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.471233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 02:00:00</td>\n",
       "      <td>-1792767705</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 03:00:00</td>\n",
       "      <td>-1792767705</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.497945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 04:00:00</td>\n",
       "      <td>-1792767705</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 05:00:00</td>\n",
       "      <td>-1792767705</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.467123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp detector_id  hour  day_of_week  is_weekend  month  \\\n",
       "0 2020-01-01 00:00:00 -1792767705     0            2           0      1   \n",
       "1 2020-01-01 02:00:00 -1792767705     2            2           0      1   \n",
       "2 2020-01-01 03:00:00 -1792767705     3            2           0      1   \n",
       "3 2020-01-01 04:00:00 -1792767705     4            2           0      1   \n",
       "4 2020-01-01 05:00:00 -1792767705     5            2           0      1   \n",
       "\n",
       "   is_holiday  is_rush_hour  is_school_holiday  congestion_index  ...  \\\n",
       "0           0             0                  1          0.471233  ...   \n",
       "1           0             0                  1          0.500685  ...   \n",
       "2           0             0                  1          0.497945  ...   \n",
       "3           0             0                  1          0.482877  ...   \n",
       "4           0             0                  1          0.467123  ...   \n",
       "\n",
       "   precipitation  visibility  is_snow  is_fog  hour_sin  hour_cos   dow_sin  \\\n",
       "0            0.0      5000.0        0       0  0.000000  1.000000  0.974928   \n",
       "1            0.0     10000.0        0       0  0.500000  0.866025  0.974928   \n",
       "2            0.0      9000.0        0       0  0.707107  0.707107  0.974928   \n",
       "3            0.0      7000.0        0       0  0.866025  0.500000  0.974928   \n",
       "4            0.0      6000.0        0       0  0.965926  0.258819  0.974928   \n",
       "\n",
       "    dow_cos  month_sin  month_cos  \n",
       "0 -0.222521        0.5   0.866025  \n",
       "1 -0.222521        0.5   0.866025  \n",
       "2 -0.222521        0.5   0.866025  \n",
       "3 -0.222521        0.5   0.866025  \n",
       "4 -0.222521        0.5   0.866025  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small[\"detector_id\"].unique()\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f202c008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\3933638425.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\3933638425.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\3933638425.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\3933638425.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\3933638425.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\3933638425.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\3933638425.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\3933638425.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\3933638425.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n"
     ]
    }
   ],
   "source": [
    "# Add lags\n",
    "\n",
    "def make_lags(df, col, lags):\n",
    "    for lag in lags:\n",
    "        df[f\"{col}_lag_{lag}h\"] = df.groupby(\"detector_id\")[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "\n",
    "weather_lags = [-i for i in range(1, 25, 8)] # Next 24 hours\n",
    "df_small = make_lags(df_small, \"temperature\", weather_lags)\n",
    "df_small = make_lags(df_small, \"precipitation\", weather_lags)\n",
    "df_small = make_lags(df_small, \"visibility\", weather_lags)\n",
    "\n",
    "# Remove rows with NaNs due to lagging\n",
    "df_small = df_small.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b57b35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125883, 30) (21549, 30) (18788, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Define parameters\n",
    "# -----------------------------\n",
    "HISTORY_OFFSETS = [0, 1, 3, 6, 12, 18, 24, 48]   # hours before t\n",
    "forecast_horizon = 24      # next 24 hours → target\n",
    "\n",
    "feature_cols_norm = [\n",
    "    \"temperature\", \"precipitation\", \"visibility\", \"congestion_index\"\n",
    "]  + [f\"temperature_lag_{lag}h\" \n",
    "    for lag in weather_lags] + [f\"precipitation_lag_{lag}h\" \n",
    "    for lag in weather_lags] + [f\"visibility_lag_{lag}h\" for lag in weather_lags]\n",
    "\n",
    "feature_cols = [\n",
    "    \"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\", \"month_sin\", \"month_cos\",\n",
    "    \"is_weekend\", \"is_holiday\", \"is_school_holiday\", \"is_rush_hour\",\n",
    "    \"temperature\", \"precipitation\", \"visibility\", \"is_snow\", \"is_fog\",\n",
    "    \"congestion_index\"\n",
    "] + [f\"temperature_lag_{lag}h\" \n",
    "    for lag in weather_lags] + [f\"precipitation_lag_{lag}h\" \n",
    "    for lag in weather_lags] + [f\"visibility_lag_{lag}h\" for lag in weather_lags]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Train / Val / Test split\n",
    "# -----------------------------\n",
    "train = df_small[df_small[\"timestamp\"] < \"2021-05-01\"].copy()\n",
    "val   = df_small[(df_small[\"timestamp\"] >= \"2021-05-01\") &\n",
    "                 (df_small[\"timestamp\"] <  \"2022-01-01\")].copy()\n",
    "test  = df_small[df_small[\"timestamp\"] >= \"2022-01-01\"].copy()\n",
    "\n",
    "print(train.shape, val.shape, test.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Normalize continuous features\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "train[feature_cols_norm] = scaler.fit_transform(train[feature_cols_norm])\n",
    "val[feature_cols_norm]   = scaler.transform(val[feature_cols_norm])\n",
    "test[feature_cols_norm]  = scaler.transform(test[feature_cols_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1fa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\2960479339.py:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for det_id, df_det in df.groupby(\"detector_id\"):\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\2960479339.py:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for det_id, df_det in df.groupby(\"detector_id\"):\n",
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\2960479339.py:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for det_id, df_det in df.groupby(\"detector_id\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "X_train_hist: (125163, 8, 25)\n",
      "Y_train: (125163, 24)\n",
      "X_val_hist: (20829, 8, 25)\n",
      "Y_val: (20829, 24)\n",
      "X_test_hist: (18068, 8, 25)\n",
      "Y_test: (18068, 24)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4) Function to create sequences\n",
    "# -----------------------------\n",
    "def create_nhits_sequences(df, feature_cols, hist_offsets, horizon):\n",
    "    X_list, Y_list, idx_list = [], [], []\n",
    "\n",
    "    for det_id, df_det in df.groupby(\"detector_id\"):\n",
    "        df_det = df_det.sort_values(\"timestamp\").reset_index(drop=False)\n",
    "        #   keep original row index      ↑↑↑\n",
    "\n",
    "        values = df_det[feature_cols].values.astype(np.float32)\n",
    "        target = df_det[\"congestion_index\"].values.astype(np.float32)\n",
    "        idx    = df_det[\"index\"].values    # original global index\n",
    "\n",
    "        n = len(df_det)\n",
    "        for t in range(max(hist_offsets), n - horizon):\n",
    "            X_list.append(values[[t-h for h in hist_offsets]])\n",
    "            Y_list.append(target[t : t+horizon])\n",
    "            idx_list.append(idx[t])   # <--- store index where prediction starts\n",
    "\n",
    "    return (\n",
    "        np.array(X_list, dtype=np.float32),\n",
    "        np.array(Y_list, dtype=np.float32),\n",
    "        np.array(idx_list, dtype=np.int64),\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Choose the model input features\n",
    "# -----------------------------\n",
    "model_features = feature_cols\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Build sequences for each split\n",
    "# -----------------------------\n",
    "X_train_hist, Y_train, train_idx = create_nhits_sequences(\n",
    "    train, model_features, HISTORY_OFFSETS, forecast_horizon)\n",
    "\n",
    "X_val_hist, Y_val, val_idx = create_nhits_sequences(\n",
    "    val, model_features, HISTORY_OFFSETS, forecast_horizon)\n",
    "\n",
    "X_test_hist, Y_test, test_idx = create_nhits_sequences(\n",
    "    test, model_features, HISTORY_OFFSETS, forecast_horizon)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train_hist:\", X_train_hist.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"train_idx:\", train_idx.shape)\n",
    "print(\"X_val_hist:\",   X_val_hist.shape)\n",
    "print(\"Y_val:\",   Y_val.shape)\n",
    "print(\"val_idx:\", val_idx.shape)\n",
    "print(\"X_test_hist:\",  X_test_hist.shape)\n",
    "print(\"Y_test:\",  Y_test.shape)\n",
    "print(\"test_idx:\", test_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80faabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NHitsDataset(Dataset):\n",
    "    def __init__(self, X_hist, Y):\n",
    "        self.X_hist = X_hist\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X_hist[idx]).float(),\n",
    "            torch.from_numpy(self.Y[idx]).float(),\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 512     # can increase if GPU is large\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    NHitsDataset(X_train_hist, Y_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    NHitsDataset(X_val_hist, Y_val),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    NHitsDataset(X_test_hist, Y_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d7a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPForecaster(nn.Module):\n",
    "    def __init__(self, input_length, num_features, horizon):\n",
    "        super().__init__()\n",
    "        in_dim = input_length * num_features\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, horizon)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten sequence into one vector per sample\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694174ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scaler, device, num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        loss_epoch = 0\n",
    "        for X_batch, Y_batch in tqdm(train_loader):\n",
    "            X_batch = X_batch.to(device, non_blocking=True)\n",
    "            Y_batch = Y_batch.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, Y_batch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "\n",
    "        train_losses.append(loss_epoch / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, Y_batch in tqdm(val_loader):\n",
    "                X_batch = X_batch.to(device, non_blocking=True)\n",
    "                Y_batch = Y_batch.to(device, non_blocking=True)\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, Y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        \n",
    "        tqdm.write(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_losses[-1]:.4f} - Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "    \n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in tqdm(loader):\n",
    "            X_batch = X_batch.to(device, non_blocking=True)\n",
    "            Y_batch = Y_batch.to(device, non_blocking=True)\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, Y_batch)\n",
    "            total_loss += loss.item()\n",
    "            preds_list.append(preds.cpu())\n",
    "\n",
    "    preds_list = torch.cat(preds_list, dim=0)\n",
    "    return preds_list, total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ca1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adib4\\AppData\\Local\\Temp\\ipykernel_35388\\2950684459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()   # AMP = automatic mixed precision\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MLPForecaster(\n",
    "    input_length=len(HISTORY_OFFSETS),\n",
    "    num_features=X_train_hist.shape[-1],\n",
    "    horizon=forecast_horizon\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "scaler = torch.amp.GradScaler(\"cuda\")  # AMP = automatic mixed precision\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "train_losses, val_losses = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    device,\n",
    "    epochs\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "preds, test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(\"Test loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584930f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_block_predictions(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Ultra-fast evaluation of block forecasts using vectorized operations.\n",
    "    Shape: Y_true, Y_pred = (N, H)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Basic metrics (vectorized) ----\n",
    "    diff = Y_true - Y_pred\n",
    "    mae  = np.mean(np.abs(diff))\n",
    "    rmse = np.sqrt(np.mean(diff * diff))\n",
    "    print(\"MAE:\", mae, \"RMSE:\", rmse)\n",
    "\n",
    "    # ---- Vectorized correlation ----\n",
    "    # Center sequences\n",
    "    Yt = Y_true - Y_true.mean(axis=1, keepdims=True)\n",
    "    Yp = Y_pred - Y_pred.mean(axis=1, keepdims=True)\n",
    "    print(\"Mean centered Y_true and Y_pred for correlation calculation.\")\n",
    "\n",
    "    # Compute numerator: cov\n",
    "    num = np.sum(Yt * Yp, axis=1)\n",
    "\n",
    "    # Denominator: std(True)*std(Pred)\n",
    "    denom = np.sqrt(np.sum(Yt * Yt, axis=1) * np.sum(Yp * Yp, axis=1))\n",
    "    print(\"Computed denominator for correlation calculation.\")\n",
    "\n",
    "    # Avoid division by zero\n",
    "    corr_per_block = np.where(denom == 0, np.nan, num / denom)\n",
    "\n",
    "    # Average correlation (ignoring NaN)\n",
    "    corr = np.nanmean(corr_per_block)\n",
    "    print(\"Correlation:\", corr)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Corr\": corr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8234f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_block_predictions(df, horizon=24, detector_id=None,\n",
    "                           years=None, months=None,\n",
    "                           true_prefix=\"future_\", pred_prefix=\"pred_\",\n",
    "                           max_blocks=10):\n",
    "    \"\"\"\n",
    "    Plot 24h forecast trajectories:\n",
    "    - t+1 ... t+horizon for each chosen block\n",
    "    - sample every 'horizon' timestamps\n",
    "    \"\"\"\n",
    "\n",
    "    df_plot = df.copy()\n",
    "\n",
    "    # ---- FILTERING ----\n",
    "    if detector_id is None:\n",
    "        detector_id = df_plot[\"detector_id\"].iloc[0]\n",
    "    df_plot = df_plot[df_plot[\"detector_id\"] == detector_id]\n",
    "\n",
    "    if years is not None:\n",
    "        df_plot = df_plot[df_plot[\"timestamp\"].dt.year.isin(years)]\n",
    "\n",
    "    if months is not None:\n",
    "        df_plot = df_plot[df_plot[\"timestamp\"].dt.month.isin(months)]\n",
    "\n",
    "    # ---- Take blocks every 'horizon' timesteps ----\n",
    "    df_blocks = df_plot.iloc[::horizon].copy()\n",
    "    df_blocks = df_blocks.head(max_blocks)\n",
    "\n",
    "    # ---- Column lists ----\n",
    "    true_cols = [f\"{true_prefix}{h}h\" for h in range(1, horizon+1)]\n",
    "    pred_cols = [f\"{pred_prefix}{h}h\" for h in range(1, horizon+1)]\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    print(\"Plotting...\")\n",
    "    for _, row in df_blocks.iterrows():\n",
    "        base_time = row[\"timestamp\"]\n",
    "        horizon_times = base_time + pd.to_timedelta(np.arange(1, horizon+1), \"h\")\n",
    "\n",
    "        plt.plot(horizon_times, row[true_cols].values,\n",
    "                 label=f\"True (start {base_time})\", alpha=0.6)\n",
    "\n",
    "        plt.plot(horizon_times, row[pred_cols].values,\n",
    "                 label=f\"Pred (start {base_time})\", alpha=0.6)\n",
    "\n",
    "    plt.title(f\"{horizon}-hour Forecast Trajectories\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Congestion Index\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_block(df, horizon=24,\n",
    "                            detector_id=None,\n",
    "                            years=None,\n",
    "                            months=None,\n",
    "                            true_prefix=\"future_\",\n",
    "                            pred_prefix=\"pred_\"):\n",
    "    \"\"\"\n",
    "    Full multi-step forecast evaluation and plotting.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Extract arrays ----\n",
    "    true_cols = [f\"{true_prefix}{h}h\" for h in range(1, horizon+1)]\n",
    "    pred_cols = [f\"{pred_prefix}{h}h\" for h in range(1, horizon+1)]\n",
    "\n",
    "    Y_true = df[true_cols].values\n",
    "    Y_pred = df[pred_cols].values\n",
    "\n",
    "    # ---- Compute metrics ----\n",
    "    metrics = evaluate_block_predictions(Y_true, Y_pred)\n",
    "\n",
    "    print(\"=== Block Forecast Evaluation ===\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    # ---- Plot ----\n",
    "    plot_block_predictions(\n",
    "        df,\n",
    "        horizon=horizon,\n",
    "        detector_id=detector_id,\n",
    "        years=years,\n",
    "        months=months,\n",
    "        true_prefix=true_prefix,\n",
    "        pred_prefix=pred_prefix\n",
    "    )\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a85b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_baseline_multi(df, window_size=5, horizon=24):\n",
    "    df_h = df[[\"detector_id\", \"timestamp\", \"congestion_index\"]].copy()\n",
    "\n",
    "    # For each horizon h = 1..24 create a future target\n",
    "    for h in range(1, horizon+1):\n",
    "        df_h[f\"future_{h}h\"] = (\n",
    "            df_h.groupby(\"detector_id\")[\"congestion_index\"]\n",
    "                .shift(-h)\n",
    "        )\n",
    "\n",
    "    # Baseline uses rolling mean of past\n",
    "    df_h[\"hist_baseline\"] = (\n",
    "        df_h.groupby(\"detector_id\")[\"congestion_index\"]\n",
    "             .rolling(window_size, min_periods=1)\n",
    "             .mean()\n",
    "             .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Expand baseline into 24 identical horizons\n",
    "    for h in range(1, horizon+1):\n",
    "        df_h[f\"pred_{h}h\"] = df_h[\"hist_baseline\"]\n",
    "\n",
    "    # Drop rows where ANY future target is missing\n",
    "    future_cols = [f\"future_{h}h\" for h in range(1, horizon+1)]\n",
    "    df_h = df_h.dropna(subset=future_cols)\n",
    "\n",
    "    return df_h\n",
    "\n",
    "\n",
    "print(\"Baseline: historical average congestion.\")\n",
    "df_historical = historical_baseline_multi(test, horizon=24)\n",
    "evaluate_and_plot_block(df_historical, horizon=24, years=[2022])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c045a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DL Model Evaluation on Test Set\")\n",
    "\n",
    "eval_df = pd.DataFrame({\n",
    "    \"row_idx\": test_idx,\n",
    "    \"timestamp\": test.loc[test_idx, \"timestamp\"].values,\n",
    "    \"detector_id\": test.loc[test_idx, \"detector_id\"].values\n",
    "})\n",
    "\n",
    "for h in range(1, forecast_horizon+1):\n",
    "    eval_df[f\"pred_{h}h\"] = preds[:, h-1].numpy()\n",
    "\n",
    "for h in range(1, forecast_horizon+1):\n",
    "    eval_df[f\"future_{h}h\"] = (\n",
    "        test.groupby(\"detector_id\")[\"congestion_index\"]\n",
    "            .shift(-h)\n",
    "            .loc[test_idx]\n",
    "            .values\n",
    "    )\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "\n",
    "\n",
    "evaluate_and_plot_block(eval_df, horizon=24, years=[2022])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
