{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee25101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c8a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRID SEARCH ANALYSIS\n",
      "================================================================================\n",
      "Total experiments: 27\n",
      "Columns: ['exp_name', 'n_blocks', 'hidden_dim', 'history', 'weather_lags', 'num_epochs', 'early_stop_epoch', 'best_val_loss', 'test_loss', 'total_time_sec', 'train_loss_ep1', 'val_loss_ep1', 'time_sec_ep1', 'train_loss_ep2', 'val_loss_ep2']...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "df = pd.read_csv(\"experiment_metrics.csv\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"grid_search_analysis\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GRID SEARCH ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()[:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d607e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1. AGGREGATE STATISTICS BY HYPERPARAMETER\n",
      "================================================================================\n",
      "\n",
      "--- By History Length ---\n",
      "                 count     mean      std     min     max  median   range\n",
      "history                                                                 \n",
      "24h_hourly           9  0.04156  0.00064  0.0406  0.0424  0.0417  0.0018\n",
      "24h_1week_daily      9  0.04240  0.00101  0.0412  0.0439  0.0420  0.0027\n",
      "24h_1week_6h         9  0.04309  0.00128  0.0421  0.0452  0.0423  0.0031\n",
      "\n",
      "--- By Weather Lags ---\n",
      "              count     mean      std     min     max  median   range\n",
      "weather_lags                                                         \n",
      "sparse_24h        9  0.04151  0.00067  0.0406  0.0424  0.0415  0.0018\n",
      "minimal           9  0.04199  0.00028  0.0415  0.0423  0.0420  0.0008\n",
      "dense_24h         9  0.04354  0.00114  0.0419  0.0452  0.0436  0.0033\n",
      "\n",
      "--- By N_Blocks ---\n",
      "          count     mean      std     min     max  median   range\n",
      "n_blocks                                                         \n",
      "6             9  0.04227  0.00119  0.0406  0.0446  0.0422  0.0040\n",
      "10            9  0.04238  0.00113  0.0409  0.0445  0.0422  0.0036\n",
      "8             9  0.04240  0.00129  0.0409  0.0452  0.0421  0.0043\n",
      "\n",
      "--- By Hidden Dim ---\n",
      "            count     mean      std     min     max  median   range\n",
      "hidden_dim                                                         \n",
      "512            18  0.04232  0.00113  0.0406  0.0446  0.0422  0.0040\n",
      "1024            9  0.04240  0.00129  0.0409  0.0452  0.0421  0.0043\n",
      "\n",
      "--- By Model Config (combined) ---\n",
      "              count     mean      std     min     max  median   range\n",
      "model_config                                                         \n",
      "6b_512h           9  0.04227  0.00119  0.0406  0.0446  0.0422  0.0040\n",
      "10b_512h          9  0.04238  0.00113  0.0409  0.0445  0.0422  0.0036\n",
      "8b_1024h          9  0.04240  0.00129  0.0409  0.0452  0.0421  0.0043\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. AGGREGATE STATISTICS BY HYPERPARAMETER\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. AGGREGATE STATISTICS BY HYPERPARAMETER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def group_stats(df, col):\n",
    "    \"\"\"Compute aggregated stats for a hyperparameter.\"\"\"\n",
    "    stats = df.groupby(col)[\"best_val_loss\"].agg([\n",
    "        \"count\", \"mean\", \"std\", \"min\", \"max\", \"median\"\n",
    "    ]).round(5)\n",
    "    stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "    return stats.sort_values(\"mean\")\n",
    "\n",
    "# By history length\n",
    "print(\"\\n--- By History Length ---\")\n",
    "history_stats = group_stats(df, \"history\")\n",
    "print(history_stats)\n",
    "\n",
    "# By weather lags\n",
    "print(\"\\n--- By Weather Lags ---\")\n",
    "weather_stats = group_stats(df, \"weather_lags\")\n",
    "print(weather_stats)\n",
    "\n",
    "# By model config (n_blocks)\n",
    "print(\"\\n--- By N_Blocks ---\")\n",
    "blocks_stats = group_stats(df, \"n_blocks\")\n",
    "print(blocks_stats)\n",
    "\n",
    "# By hidden_dim\n",
    "print(\"\\n--- By Hidden Dim ---\")\n",
    "hidden_stats = group_stats(df, \"hidden_dim\")\n",
    "print(hidden_stats)\n",
    "\n",
    "# By model config combined\n",
    "df[\"model_config\"] = df[\"n_blocks\"].astype(str) + \"b_\" + df[\"hidden_dim\"].astype(str) + \"h\"\n",
    "print(\"\\n--- By Model Config (combined) ---\")\n",
    "model_stats = group_stats(df, \"model_config\")\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5cf1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "2. GENERATING INTERACTION HEATMAPS\n",
      "================================================================================\n",
      "Saved: grid_search_analysis\\heatmaps_interactions.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. INTERACTION HEATMAPS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. GENERATING INTERACTION HEATMAPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# History Ã— Weather Lags\n",
    "pivot1 = df.pivot_table(values=\"best_val_loss\", index=\"history\", columns=\"weather_lags\", aggfunc=\"mean\")\n",
    "sns.heatmap(pivot1, annot=True, fmt=\".4f\", cmap=\"RdYlGn_r\", ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Best Val Loss: History Ã— Weather Lags\")\n",
    "\n",
    "# History Ã— Model Config\n",
    "pivot2 = df.pivot_table(values=\"best_val_loss\", index=\"history\", columns=\"model_config\", aggfunc=\"mean\")\n",
    "sns.heatmap(pivot2, annot=True, fmt=\".4f\", cmap=\"RdYlGn_r\", ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Best Val Loss: History Ã— Model Config\")\n",
    "\n",
    "# Weather Lags Ã— Model Config\n",
    "pivot3 = df.pivot_table(values=\"best_val_loss\", index=\"weather_lags\", columns=\"model_config\", aggfunc=\"mean\")\n",
    "sns.heatmap(pivot3, annot=True, fmt=\".4f\", cmap=\"RdYlGn_r\", ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Best Val Loss: Weather Lags Ã— Model Config\")\n",
    "\n",
    "# Train-Val Gap (overfitting indicator)\n",
    "df[\"train_val_gap\"] = df[\"val_loss_ep15\"] - df[\"train_loss_ep15\"] if \"val_loss_ep15\" in df.columns else None\n",
    "# Use last available epoch\n",
    "last_epoch = df[\"num_epochs\"].max()\n",
    "df[\"final_train_loss\"] = df[[f\"train_loss_ep{i}\" for i in range(1, last_epoch+1) if f\"train_loss_ep{i}\" in df.columns]].apply(\n",
    "    lambda row: row.dropna().iloc[-1] if not row.dropna().empty else None, axis=1\n",
    ")\n",
    "df[\"final_val_loss\"] = df[[f\"val_loss_ep{i}\" for i in range(1, last_epoch+1) if f\"val_loss_ep{i}\" in df.columns]].apply(\n",
    "    lambda row: row.dropna().iloc[-1] if not row.dropna().empty else None, axis=1\n",
    ")\n",
    "df[\"train_val_gap\"] = df[\"final_val_loss\"] - df[\"final_train_loss\"]\n",
    "\n",
    "pivot4 = df.pivot_table(values=\"train_val_gap\", index=\"weather_lags\", columns=\"history\", aggfunc=\"mean\")\n",
    "sns.heatmap(pivot4, annot=True, fmt=\".4f\", cmap=\"RdYlGn_r\", ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Train-Val Gap (Overfitting): Weather Lags Ã— History\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"heatmaps_interactions.png\", dpi=150)\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'heatmaps_interactions.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d392b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "3. EARLY LEARNING DYNAMICS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "--- Learning Behavior by Config ---\n",
      "learning_behavior  Healthy\n",
      "weather_lags              \n",
      "dense_24h                9\n",
      "minimal                  9\n",
      "sparse_24h               9\n",
      "Saved: grid_search_analysis\\training_curves_by_config.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. EARLY LEARNING DYNAMICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. EARLY LEARNING DYNAMICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract early epoch losses\n",
    "early_epochs = 5\n",
    "early_train_cols = [f\"train_loss_ep{i}\" for i in range(1, early_epochs+1)]\n",
    "early_val_cols = [f\"val_loss_ep{i}\" for i in range(1, early_epochs+1)]\n",
    "\n",
    "# Check for underfitting (both high) vs overfitting (train low, val high)\n",
    "df[\"early_train_mean\"] = df[early_train_cols].mean(axis=1)\n",
    "df[\"early_val_mean\"] = df[early_val_cols].mean(axis=1)\n",
    "df[\"early_gap\"] = df[\"early_val_mean\"] - df[\"early_train_mean\"]\n",
    "\n",
    "# Classify learning behavior\n",
    "def classify_learning(row):\n",
    "    if row[\"early_gap\"] > 0.01:\n",
    "        return \"Overfitting\"\n",
    "    elif row[\"early_train_mean\"] > 0.06:\n",
    "        return \"Underfitting\"\n",
    "    else:\n",
    "        return \"Healthy\"\n",
    "\n",
    "df[\"learning_behavior\"] = df.apply(classify_learning, axis=1)\n",
    "\n",
    "print(\"\\n--- Learning Behavior by Config ---\")\n",
    "behavior_pivot = pd.crosstab(df[\"weather_lags\"], df[\"learning_behavior\"])\n",
    "print(behavior_pivot)\n",
    "\n",
    "# Plot training curves comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Get epoch columns\n",
    "max_ep = df[\"num_epochs\"].max()\n",
    "train_cols = [f\"train_loss_ep{i}\" for i in range(1, max_ep+1) if f\"train_loss_ep{i}\" in df.columns]\n",
    "val_cols = [f\"val_loss_ep{i}\" for i in range(1, max_ep+1) if f\"val_loss_ep{i}\" in df.columns]\n",
    "\n",
    "# Plot by weather_lags\n",
    "for ax, group_col, title in zip(axes, [\"weather_lags\", \"history\", \"model_config\"], \n",
    "                                  [\"By Weather Lags\", \"By History\", \"By Model Config\"]):\n",
    "    for name, group in df.groupby(group_col):\n",
    "        val_losses = group[val_cols].mean()\n",
    "        ax.plot(range(1, len(val_losses)+1), val_losses, label=name, marker='o', markersize=3)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Val Loss\")\n",
    "    ax.set_title(f\"Validation Loss Curves {title}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"training_curves_by_config.png\", dpi=150)\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'training_curves_by_config.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b448931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "4. OVERFITTING ANALYSIS\n",
      "================================================================================\n",
      "Saved: grid_search_analysis\\overfitting_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. OVERFITTING ANALYSIS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. OVERFITTING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of train-val gap by weather_lags\n",
    "gap_by_weather = df.groupby(\"weather_lags\")[\"train_val_gap\"].agg([\"mean\", \"std\"])\n",
    "gap_by_weather.plot(kind=\"bar\", y=\"mean\", yerr=\"std\", ax=axes[0], color=\"coral\", capsize=4)\n",
    "axes[0].set_title(\"Train-Val Gap by Weather Lags (Higher = More Overfitting)\")\n",
    "axes[0].set_ylabel(\"Gap (Val - Train)\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Bar chart of train-val gap by history\n",
    "gap_by_history = df.groupby(\"history\")[\"train_val_gap\"].agg([\"mean\", \"std\"])\n",
    "gap_by_history.plot(kind=\"bar\", y=\"mean\", yerr=\"std\", ax=axes[1], color=\"steelblue\", capsize=4)\n",
    "axes[1].set_title(\"Train-Val Gap by History Length\")\n",
    "axes[1].set_ylabel(\"Gap (Val - Train)\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"overfitting_analysis.png\", dpi=150)\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'overfitting_analysis.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2821be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "5. EFFICIENCY ANALYSIS (Val Loss vs Training Time)\n",
      "================================================================================\n",
      "Saved: grid_search_analysis\\efficiency_pareto.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. EFFICIENCY ANALYSIS (Pareto Frontier)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. EFFICIENCY ANALYSIS (Val Loss vs Training Time)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Scatter plot colored by weather_lags\n",
    "colors = {\"minimal\": \"green\", \"sparse_24h\": \"blue\", \"dense_24h\": \"red\"}\n",
    "markers = {\"24h_hourly\": \"o\", \"24h_1week_daily\": \"s\", \"24h_1week_6h\": \"^\"}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    ax.scatter(row[\"total_time_sec\"], row[\"best_val_loss\"], \n",
    "               c=colors.get(row[\"weather_lags\"], \"gray\"),\n",
    "               marker=markers.get(row[\"history\"], \"o\"),\n",
    "               s=100, alpha=0.7)\n",
    "\n",
    "# Add Pareto frontier\n",
    "sorted_df = df.sort_values(\"total_time_sec\")\n",
    "pareto_points = []\n",
    "best_val = float(\"inf\")\n",
    "for _, row in sorted_df.iterrows():\n",
    "    if row[\"best_val_loss\"] < best_val:\n",
    "        pareto_points.append((row[\"total_time_sec\"], row[\"best_val_loss\"]))\n",
    "        best_val = row[\"best_val_loss\"]\n",
    "\n",
    "if pareto_points:\n",
    "    pareto_x, pareto_y = zip(*pareto_points)\n",
    "    ax.plot(pareto_x, pareto_y, 'k--', linewidth=2, label=\"Pareto Frontier\")\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='minimal'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='sparse_24h'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='dense_24h'),\n",
    "    Line2D([0], [0], marker='o', color='gray', markersize=10, label='24h_hourly'),\n",
    "    Line2D([0], [0], marker='s', color='gray', markersize=10, label='24h_1week_daily'),\n",
    "    Line2D([0], [0], marker='^', color='gray', markersize=10, label='24h_1week_6h'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"upper right\")\n",
    "\n",
    "ax.set_xlabel(\"Total Training Time (seconds)\")\n",
    "ax.set_ylabel(\"Best Validation Loss\")\n",
    "ax.set_title(\"Efficiency: Val Loss vs Training Time\\n(Color=Weather Lags, Shape=History)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"efficiency_pareto.png\", dpi=150)\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'efficiency_pareto.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6eedeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "6. TOP 10 CONFIGURATIONS\n",
      "================================================================================\n",
      "                                   exp_name          history weather_lags model_config  best_val_loss  train_val_gap  total_time_sec  num_epochs\n",
      "0        grid/b6_h512_24h_hourly_sparse_24h       24h_hourly   sparse_24h      6b_512h         0.0406         0.0023             693          15\n",
      "1       grid/b8_h1024_24h_hourly_sparse_24h       24h_hourly   sparse_24h     8b_1024h         0.0409         0.0048            1576          15\n",
      "2       grid/b10_h512_24h_hourly_sparse_24h       24h_hourly   sparse_24h     10b_512h         0.0409         0.0021            1079          15\n",
      "3   grid/b6_h512_24h_1week_daily_sparse_24h  24h_1week_daily   sparse_24h      6b_512h         0.0412         0.0035             810          15\n",
      "4          grid/b8_h1024_24h_hourly_minimal       24h_hourly      minimal     8b_1024h         0.0415        -0.0001            1543          15\n",
      "5  grid/b8_h1024_24h_1week_daily_sparse_24h  24h_1week_daily   sparse_24h     8b_1024h         0.0415         0.0065            1869          15\n",
      "6  grid/b10_h512_24h_1week_daily_sparse_24h  24h_1week_daily   sparse_24h     10b_512h         0.0416         0.0031            1289          15\n",
      "7          grid/b10_h512_24h_hourly_minimal       24h_hourly      minimal     10b_512h         0.0417        -0.0008            1059          15\n",
      "8           grid/b6_h512_24h_hourly_minimal       24h_hourly      minimal      6b_512h         0.0418        -0.0014             954          15\n",
      "9         grid/b6_h512_24h_hourly_dense_24h       24h_hourly    dense_24h      6b_512h         0.0419         0.0089             821          15\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. TOP CONFIGURATIONS SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. TOP 10 CONFIGURATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_configs = df.nsmallest(10, \"best_val_loss\")[\n",
    "    [\"exp_name\", \"history\", \"weather_lags\", \"model_config\", \n",
    "     \"best_val_loss\", \"train_val_gap\", \"total_time_sec\", \"num_epochs\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(top_configs.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b527edb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "7. RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "ðŸ† BEST VALIDATION LOSS:\n",
      "   Config: grid/b6_h512_24h_hourly_sparse_24h\n",
      "   Val Loss: 0.04060\n",
      "   Gap: 0.00230\n",
      "\n",
      "âš–ï¸ LEAST OVERFITTING:\n",
      "   Config: grid/b6_h512_24h_hourly_minimal\n",
      "   Val Loss: 0.04180\n",
      "   Gap: -0.00140\n",
      "\n",
      "ðŸ“Š AGGREGATE INSIGHTS:\n",
      "   â€¢ Best History: 24h_hourly (mean val: 0.04156)\n",
      "   â€¢ Best Weather Lags: sparse_24h (mean val: 0.04151)\n",
      "   â€¢ Best Model: 6b_512h (mean val: 0.04227)\n",
      "\n",
      "Saved summary to: grid_search_analysis\\summary_report.txt\n",
      "\n",
      "All plots saved to: grid_search_analysis/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. RECOMMENDATIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best by each criterion\n",
    "best_val = df.loc[df[\"best_val_loss\"].idxmin()]\n",
    "best_efficient = df.loc[(df[\"best_val_loss\"] / df[\"total_time_sec\"]).idxmin()] if df[\"total_time_sec\"].notna().any() else None\n",
    "least_overfit = df.loc[df[\"train_val_gap\"].idxmin()]\n",
    "\n",
    "print(f\"\\nðŸ† BEST VALIDATION LOSS:\")\n",
    "print(f\"   Config: {best_val['exp_name']}\")\n",
    "print(f\"   Val Loss: {best_val['best_val_loss']:.5f}\")\n",
    "print(f\"   Gap: {best_val['train_val_gap']:.5f}\")\n",
    "\n",
    "print(f\"\\nâš–ï¸ LEAST OVERFITTING:\")\n",
    "print(f\"   Config: {least_overfit['exp_name']}\")\n",
    "print(f\"   Val Loss: {least_overfit['best_val_loss']:.5f}\")\n",
    "print(f\"   Gap: {least_overfit['train_val_gap']:.5f}\")\n",
    "\n",
    "# Aggregate recommendation\n",
    "print(\"\\nðŸ“Š AGGREGATE INSIGHTS:\")\n",
    "print(f\"   â€¢ Best History: {history_stats.index[0]} (mean val: {history_stats.iloc[0]['mean']:.5f})\")\n",
    "print(f\"   â€¢ Best Weather Lags: {weather_stats.index[0]} (mean val: {weather_stats.iloc[0]['mean']:.5f})\")\n",
    "print(f\"   â€¢ Best Model: {model_stats.index[0]} (mean val: {model_stats.iloc[0]['mean']:.5f})\")\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = output_dir / \"summary_report.txt\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(\"GRID SEARCH ANALYSIS SUMMARY\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(\"TOP 10 CONFIGURATIONS:\\n\")\n",
    "    f.write(top_configs.to_string() + \"\\n\\n\")\n",
    "    f.write(\"AGGREGATE STATS BY HISTORY:\\n\")\n",
    "    f.write(history_stats.to_string() + \"\\n\\n\")\n",
    "    f.write(\"AGGREGATE STATS BY WEATHER LAGS:\\n\")\n",
    "    f.write(weather_stats.to_string() + \"\\n\\n\")\n",
    "    f.write(\"AGGREGATE STATS BY MODEL CONFIG:\\n\")\n",
    "    f.write(model_stats.to_string() + \"\\n\\n\")\n",
    "\n",
    "print(f\"\\nSaved summary to: {summary_path}\")\n",
    "print(f\"\\nAll plots saved to: {output_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
